
\documentclass[]{report}
\voffset=-1.5cm
\oddsidemargin=0.0cm
\textwidth = 480pt


\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{framed}
\usepackage{multicol}
%\usepackage[paperwidth=21cm, paperheight=29.8cm]{geometry}
%\usepackage[angle=0,scale=1,color=black,hshift=-0.4cm,vshift=15cm]{background}
%\usepackage{multirow}
\usepackage{enumerate}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{color}
\usepackage{multirow}
\usepackage{eurosym}
\usepackage{framed}

%\input def.tex
%\input dsdef.tex
%\input rgb.tex

%\newcommand \la{\lambda}
%\newcommand \al{a}
%\newcommand \be{b}
\newcommand \x{\overline{x}}
\newcommand \y{\overline{y}}
\begin{document}

II – 75

%====================================================================================%
\section{7. Infinite Games.}
In this Chapter, we treat infinite two-person, zero-sum games. These are games
(X, Y, A), in which at least one of the strategy sets, X and Y , is an infinite set. The
famous example of Exercise 4.7.3, he-who-chooses-the-larger-integer-wins, shows that an
infinite game may not have a value. Even worse, the example of Exercise 4.7.5 shows
that the notion of a value may not even make sense in infinite games without further
restrictions. This latter problem will be avoided when we assume that the function A(x, y)
is either bounded above or bounded below.

%====================================================================================%
\subsection{7.1 The Minimax Theorem for Semi-Finite Games.} The minimax theorem for
finite games states that every finite game has a value and both players have optimal mixed
strategies. The first theorem below generalizes this result to the case where only one of
the players has a finite number of pure strategies. The conclusion is that the value exists
and the player with a finite number of pure strategies has an optimal mixed strategy. But
first we must discuss mixed strategies and near optimal strategies for infinite games.

%====================================================================================%
\noindent \textbf{Mixed Strategies for Infinite Games:}\\ First note that for infinite games, the notion
of a mixed strategy is somewhat open to choice. Suppose the strategy set, Y , of Player
II is infinite. The simplest choice of a mixed strategy is a finite distribution over Y .
This is a distribution that gives all its probability to a finite number of points. Such a
distribution is described by a finite number of points of Y , say y1, y2,...,yn, and a set of
probabilities, q1, q2,...,qn summing to one with the understanding that point yj is chosen
with probability qj . We will denote the set of finite distributions on Y by Y ∗
F .

%====================================================================================%
When Y is an interval of the real line, we may allow as a mixed strategy any distribution
over Y given by its distribution function, F(z). Here, F(z) represents the probabiity
that the randomly chosen pure strategy, y, is less than or equal to z. The advantage of
enlarging the set of mixed strategies is that it then becomes more likely that an optimal
mixed strategy will exist. The payoff for using such a strategy is denoted by A(x, F) for
$x \in X$.
\noindent \textbf{Near Optimal Strategies for Infinite Games:} When a game has a finite value and
an optimal strategy for a player does not exist, that player must be content to choosing
a strategy that comes within  of achieving the value of the game for some small  > 0.
Such a strategy is called an -optimal strategy and was discussed in Chapter 6.
In infinite games, we allow the value to be +∞ or −∞. For example, the value is +∞
if for every number B, however large, there exists a mixed strategy p for Player I such
that A(p, y) ≥ B for all $y \in Y$ . A simple example would be: X = [0,∞), Y arbitrary, and
A(x, y) = x independent of y. The value is +∞, since for any B, Player I can guarantee
winning at least B by choosing any x ≥ B. Such a strategy might be called B-optimal.
We will refer to both -optimal and B-optimal strategies as near optimal strategies.
The Semi-Finite Minimax Theorem. For finite X = {x1,...,xm}, we denote the set
of mixed strategies of Player I as usual by X∗. If Player I uses p ∈ X∗ and Player II uses
II – 76
q ∈ Y ∗
F , then the average payoff is denoted by
A(p, q) = 

i


j
piA(xi , yj )qj . (1)
We denote the set of mixed strategies of Player II by Y ∗, but we shall always assume that
Y ∗
F ⊂ Y ∗.

%====================================================================================%
Consider the semi-finite two-person zero-sum game, (X, Y, A), in which X is a finite
set, Y is an arbitrary set, and A(x, y) is the payoff function — the winnings of Player I
if he chooses $x \in X$ and Player II chooses $y \in Y$ . To avoid the the possibility that the
average payoff does not exist or that the value might be −∞, we assume that the payoff
function, A(x, y), is bounded below. By bounded below, we mean that there is a number
M such that A(x, y) > M for all $x \in X$ and all $y \in Y$ . This assumption is weak from
the point of view of utility theory because, as mentioned in Appendix 1, it is customary
to assume that utility is bounded.
It is remarkable that the minimax theorem still holds in this situation. Specifically
the value exists and Player I has a minimax strategy. In addition, for every  > 0, Player
II has an -minimax strategy within Y ∗
F .

%====================================================================================%
Theorem 7.1. If X is finite and A is bounded below, then the game (X, Y, A) has a finite
value and Player I has an optimal mixed strategy. In addition, if X has m elements, then
Player II has near optimal strategies that give weight to at most m points of Y .
This theorem is valid without the asumption that A is bounded below provided Player
II is restricted to finite strategies, i.e. Y ∗ = Y ∗
F . See Exercise 1. However, the value may
be −∞, and the notion of near optimal strategies must be extended to this case.
By symmetry, if Y is finite and A is bounded above, then the game (X, Y, A) has a
value and Player II has an optimal mixed strategy.
Solving Semi-Finite Games. Here are two methods that may be used to solve semi-
finite games. We take X to be the finite set, X = {x1,...,xm}.

%====================================================================================%
METHOD 1. The first method is similar to the method used to solve 2 × n games
presented in Section 2.2. For each fixed $y \in Y$ , the payoff, A(p, y), is a linear function of p
on the set X∗ = {p = (p1,...,pm) : pi ≥ 0,
pi = 1}. The optimal strategy for Player I is
that value of p that maximizes the lower envelope, f(p) ≡ infy∈Y A(p, y). Note that f(p),
being the infimum of a collection of concave continuous (here linear) functions, is concave
and continuous on X∗. Since X∗ is compact, there exists a p at which the maximum of f(p)
is attained. General methods for solving concave maximization problems are available.
Example 1. Player I chooses x ∈ {x1, x2}, Player II chooses y ∈ [0, 1], and the payoff
is
A(x, y) =  y if x = x1
(1 − y)2 if x = x2
II – 77
A(p, y)
0 1 .553
0
1
p
y=0
y=1/4
y=.382
y=1/2
y=2/3
y=1
Figure 7.1
Let p denote the probability that Player I chooses x1 and let p = (p, 1−p). For a given
choice of $y \in Y$ by Player II, the expected payoff to Player I is A(p, y) = py+(1−p)(1−y)2.
The minimum of A(p, y) over y occurs at p − (1 − p)2(1 − y) = 0, or y = (2 − 3p)/(2 − 2p);
except that for p > 2/3, the minimum occurs at y = 0. So, the lower envelope is
f(p) = min
y
A(p, y) =  p 4−5p
4−4p
if p ≤ 2/3
1 − p if p ≥ 2/3
The maximum of this function occurs for p ≤ 2/3, and is easily found to be p = 1 −
(1/
√5) = .553 .... The optimal strategy for Player II occurs at that value of y for which
the slope of A(p, y) (as a function of p) is zero. This occurs when y = (1 − y)2. We find
y = (3 − √5)/2 = .382 ... is an optimal pure strategy for Player II. This is also the value
of the game. See Figure 7.1, in which the lower envelope is shown as the thick line.
METHOD 2: S-GAMES. (Blackwell and Girshick (1954).) Let X = {1, 2,...,m},
and let S be a non-empty convex subset of m-dimensional Euclidean space, Rm, and
assume that S is bounded below. Player II chooses a point s = (s1,...,sm) in S, and
simultaneously Player I chooses a coordinate i ∈ X. Then Player II pays si to Player I.
Such games are called S-games.
%=======================================================================%
This game arises from the semi-finite game (X, Y, A) with X = {1, 2,...,m} by letting
S0 = {s = (A(1, y),...,A(m, y)) : $y \in Y$ }. Choosing $y \in Y$ is equivalent to choosing
s ∈ S0. Although S0 is not necessarily convex, Player II can, by using a mixed strategy,
choose a probability mixture of points in S0. This is equivalent to choosing a point s in
S, where S is the convex hull of S0.
To solve the game, let Wc denote the “wedge” at the point (c, . . . , c) on the diagonal
in Rm,
Wc = {s : si ≤ c for all i = 1,...,m}.
II – 78
Start with some c such that the wedge Wc contains no points of s, i.e. Wc ∩ S = ∅. Such
a value of c exists from the assumption that S is bounded below. Now increase c and so
push the wedge up to the right until it just touches S. See Figure 7.2(a). This gives the
value of the game:
v = sup{c : Wc ∩ S = ∅}.
Any point s ∈ Wv ∩ S is an optimal pure strategy for Player II. It guarantees that II will
lose no more than v. Such a strategy will exist if S is closed. If Wv ∩ S is empty, then
any point s ∈ Wv+ ∩ S is an -optimal strategy for Player II. The point (v,... , v) is not
necessarily an optimal strategy for Player II. The optimal strategy could be on the side of
the wedge as in Figure 7.2(b).
S
Wv
(v,v)
(0,0)
W S v
(v,v)
(0,0)
Figure 7.2(a) Figure 7.2(b)
To find an optimal strategy for Player I, first find a plane that separates Wv and S,
i.e. that keeps Wv on one side and S on the other. Then find the vector perpendicular to
this plane, i.e. the normal vector. The optimal strategy of Player I is the mixed strategy
with components proportional to this normal vector.
Example 2. Let S be the set S = {(y1, y2) : y1 ≥ 0, y2 ≥ (1−y1)2}. This is essentially
the same as Example 1. The wedge first hits S at the vertex (v, v) when v = (1−v)2. The
solution to this equation gives the value, v = (3 − √5)/2. The point (v, v) is optimal for
Player II. To find Player I’s optimal strategy, we find the slope of the curve y2 = (1−y1)2 at
the point (v, v). The slope of the curve is −2(1−y1), which at y1 = v is −2(1−v)=1−√5.
The slope of the normal is the negative of the reciprocal of this, namely 1/(
√5 − 1). So
p2/p1 = 1/(
√5 − 1), and since p1 + p2 = 1, we find p2(
√5 − 1) = 1 − p2, or p2 = 1/
√5 and
p1 = 1 − (1/
√
5). as found in Example 1.
%===================================================%
\subsection{7.2 Continuous Games.} The simplest extension of the minimax theorem to a more
general case is to assume that X and Y are compact subsets of Euclidean spaces, and that
A(x, y) is a continuous function of x and y. To conclude that optimal strategies for the
players exist, we must allow arbitrary distribution functions on X and Y . Thus if X is a
compact subset of m-dimensional space Rm, X∗ is taken to be the set of all distributions
II – 79
on Rm that give probability 0 to the complement of X. Similarly if Y is n-dimensional,
Y ∗ is taken to be the set of all distributions on Rn giving weight 0 to the complement of
Y . Then A is extended to be defined on X∗ × Y ∗ by
A(P, Q) =   A(x, y) dP(x) dQ(y)
Theorem 7.2. If X and Y are compact subsets of Euclidean space and if A(x, y) is a
continuous function of x and y, then the game has a value, v, and there exist optimal
strategies for the players; that is, there is a P0 ∈ X∗ and a Q0 ∈ Y ∗ such that
A(P, Q0) ≤ v ≤ A(P0, Q) for all P ∈ X∗ and Q ∈ Y ∗.
Example 1. Consider the game (X, Y, A), where X = Y = [0, 1], the unit interval,
and
A(x, y) =  g(x − y) if 0 ≤ y ≤ x
g(1 + x − y) if 0 ≤ x<y ≤ 1,
where g is a continuous function defined on [0, 1], with g(0) = g(1). Here, both X∗ and
Y ∗ are the set of probability distributions on the unit interval.

%====================================================================================%
Since X and Y are compact and A(x, y) is continuous on [0, 1]2, we have by Theorem
7.2, that the game has a value and the players have optimal strategies. Let us check that
the optimal strategies for both players is the uniform distribution on [0, 1]. If Player I uses
a uniform on [0,1] to choose x and Player II uses the pure strategy y ∈ [0, 1], the expected
payoff to Player I is
 1
0
A(x, y) dx =
 y
0
g(1 + x − y) dx +
 1
y
g(x − y) dx
=
 1
1−y
g(u) du +
 1−y
0
g(u) du =
 1
0
g(u) du
Since this is independent of y, Player I’s strategy is an equalizer strategy, guaranteeing
him an average payoff of  1
0 g(u) du. Clearly, the same analysis gives Player II this same
amount if he chooses y at random according to a uniform distribution on [0,1]. So these
strategies are optimal and the value is v =  1
0 g(u) du. It may be noticed that this example
is a continuous version of a Latin square game. In fact the same solution holds even if g
in not continuous. One only needs g to be integrable on [0, 1].

%====================================================================================%
A One-Sided Minimax Theorem. In the way that Theorem 7.1 generalized the
finite minimax theorem by allowing Y to be an arbitrary set, Theorem 7.2 may be generalized
to allow Y to be arbitrary, provided we keep the compactness condition on X.
The continuity condition may be weakened to assuming only that A(x, y) is a continuous
function of x for every $y \in Y$ . And even this can be weakened to assuming that A(x, y) is
only upper semi-continuous in x for every $y \in Y$ .
II – 80
A function f(x) defined on X is upper semi-continuous at a point x0 ∈ X, if for any
sequence x1, x2,... of points in X such that limn→∞ xn = x0, we have limn→∞ f(xn) ≤
f(x0). It is upper semi-continuous (usc) on X if it is upper semicontinuous at every point
of X. A function f(x) is lower semi-continuous (lsc) if the above inequality is changed to
limn→∞ f(xn) ≥ f(x0), or equivalently, if the function, −f(x), is upper semi-continuous.
As an example, the function
f(x) =  0 if x < 0
a if x = 0
1 if x > 0
is usc if a ≥ 1 and lsc if a ≤ 0. It is neither usc nor lsc if 0 <a< 1.
Theorem 7.3. If X is a compact subset of Euclidean space, and if A(x, y) is an upper
semi-continuous function of $x \in X$ for all $y \in Y$ and if A is bounded below (or if Y ∗ is the
set of finite mixtures), then the game has a value, Player I has an optimal strategy in X∗,
and for every  > 0 Player II has an -optimal strategy giving weight to a finite number of
points.
Similarly from Player II’s viewpoint, if Y is a compact subset of Euclidean space, and
if A(x, y) is a lower semi-continuous function of $y \in Y$ for all $x \in X$ and if A is bounded
above (or if X∗ is the set of finite mixtures), then the game has a value and Player II has
an optimal strategy in Y ∗.

%====================================================================================%
Example 2. Player I chooses a number in [0,1] and Player II tries to guess what it
is. Player I wins 1 if Player II’s guess is off by at least 1/3; otherwise, there is no payoff.
Thus, X = Y = [0, 1], and A(x, y) =  1 if |x − y| ≥ 1/3
0 if |x − y| < 1/3. Although the payoff
function is not continuous, it is upper semi-continuous in x for every $y \in Y$ . Thus the
game has a value and Player I has an optimal mixed strategy.
If we change the payoff so that Player I wins 1 if Player II’s guess is off by more than
1/3, then A(x, y) =  1 if |x − y| > 1/3
0 if |x − y| ≤ 1/3. This is no longer upper semi-continuous in x
for fixed y; instead it is lower semi-continuous in y for each $x \in X$. This time, the game
has a value and Player II has an optimal mixed strategy.

%====================================================================================%
\subsection{7.3 Concave Games and Convex Games.} If in Theorem 7.2, we add the assumption
that the payoff function A(x, y) is concave in x for all y or convex in y for all x, then
we can conclude that one of the players has an optimal pure strategy, which is usually easy
to find. Here is a one-sided version that complements Theorem 7.3. A good reference for
these ideas is the book of Karlin (1959), vol. 2.
%========================================================================%
Theorem 7.4. Let (X, Y, A) be a game with Y arbitrary, X a compact, convex subset of
Rm, and A(x, y) bounded below. If A(x, y) is a concave function of $x \in X$ for all $y \in Y$ ,
then the game has a value and Player I has an optimal pure strategy. Moreover, Player II
has an -optimal strategy that is a mixture of at most m + 1 pure strategies.
II – 81
The dual statement for convex functions is: If Y is compact and convex in Rn, and
if A is bounded above and is convex in $y \in Y$ for all $x \in X$, then the game has a value,
Player II has an optimal pure strategy and Player I has -optimal strategies giving weight
to at most n + 1 points..

%====================================================================================%
These games may be solved by a method similar to Method 1 of Section 7.1. Let’s
see how to find the optimal strategy of Player II in the convex functio case. Let g(y) =
supx A(x, y) be the upper envelope. Then g(y) is finite since A is bounded above. It is also
convex since the supremum of any set of convex functions is convex. Then since convex
functions defined on a compact set attain their maximum, there exists a point y^{\ast} at which
g(y) takes on its maximum value, so that
\[A(x, y^{\ast}) ≤ max x A(x, y^{\ast}) = g(y^{\ast}) for all $x \in X$.\]
Any such point is an optimal pure strategy for Player II. By choosing y^{\ast}, Player II will
lose no more than g(y^{\ast}) no matter what Player I does. Player I’s optimal strategy is
more complex to describe in general; it gives weight only to points that play a role in the
upper envelope at the point y^{\ast}. These are points x such that A(x, y) is tangent (or nearly
tangent if only -optimal strategies exist) to the surface g(y) at y^{\ast}. It is best to consider
examples.

%====================================================================================%
Example 1. Estimation. Player I chooses a point $x \in X$ = [0, 1], and Player II
tries to choose a point $y \in Y$ = [0, 1] close to x. Player II loses the square of the distance
from x to y: A(x, y)=(x − y)2. This is a convex function of y ∈ [0, 1] for all $x \in X$.
For any x, A(x, y) is bounded above by either A(0, y) or A(1, y) so the upper envelope
is g(y) = max{A(0, y), A(1, y)} = max{y2,(1 − y)2}. This is minimized at y^{\ast} = 1/2. If
Player II uses y^{\ast}, she is guaranteed to lose no more than g(y^{\ast})=1/4.
Since x = 0 and x = 1 are the only two pure strategies influencing the upper envelope,
and since y2 and (1−y)2 have slopes at y^{\ast} that are equal in absolute value but opposite in
sign, Player I should mix 0 and 1 with equal probability. This mixed strategy has convex
payoff (1/2)(A(0, y) + A(1, y)) with slope zero at y^{\ast}. Player I is guaranteed winning at
least 1/4, so v = 1/4 is the value of the game. The pure strategy y^{\ast} is optimal for Player
II and the mixed strategy, 0 with probability 1/2 and 1 with probability 1/2, is optimal for
Player I. In this example, n = 1, and Player I’s optimal strategy mixes 2 = n + 1 points.
Theorem 7.4 may also be stated with the roles of the players reversed. If Y is arbitrary,
and if X is a compact subset of Rm and if A(x, y) is bounded below and concave in $x \in X$
for all $y \in Y$ , then Player I has an optimal pure strategy, and Player II has an -optimal
strategy mixing at most m +1 pure strategies. It may also happen that A(x, y) is concave
in x for all y, and convex in y for all x. In that case, both players have optimal pure
strategies as in the following example.
%=======================================================================%
\subsubsection{Example 2. A Convex-Concave Game.} Suppose X = Y = [0, 1], and A(x, y) =
−2x2 +4xy+y2 −2x−3y+1. The payoff is convex in y for all x and concave in x for all y.
Therefore, both players have pure optimal strategies, say x0 and y0. If Player II uses y0,
II – 82
then A(x, y0) must be maximized by x0. To find maxx∈[0,1] A(x, y0) we take a derivative
with respect to x: ∂
∂xA(x, y0) = −4x + 4y0 − 2. So
x0 =
 y0 − (1/2) if y0 > 1/2
0 if y0 ≤ 1/2
Similarly, if Player I uses x0, then A(x0, y) is minimized by y0. Since ∂
∂y A(x0, y)=4x0 +
2y − 3, we have
y0 =
⎧
⎨
⎩
1 if x0 ≤ 1/4
(1/2)(3 − 4x0) if 1/4 ≤ x0 ≤ 3/4
0 if x0 ≥ 3/4.
These two equations are satisfied only if x0 = y0 − (1/2) and y0 = (1/2)(3 − 4x0). It is
then easily found that x0 = 1/3 and y0 = 5/6. The value is A(x0, y0) = −7/12.
It may be easier here to find the saddle-point of the surface, z = −2x2 + 4xy + y2 −
2x − 3y + 1, and if the saddle-point is in the unit square, then that is the solution. But
the method used here shows what must be done in general.

%====================================================================================%
\subsection{7.4 Solving Games.} There are many interesting games that are more complex and
that require a good deal of thought and ingenuity to find solutions. There is one tool
for solving such games that is basic. This is the infinite game analog of the principle of
indifference given in Chapter 3: Search for strategies that make the opponent indifferent
among all his “good” pure strategies.

%====================================================================================%
To be more specific, consider the game (X, Y, A) with X = Y = [0, 1] and A(x, y)
continuous. Let v denote the value of the game and let P denote the distribution that
represents the optimal strategy for Player I. Then, A(P, y) must be equal to v for all
“good” y, which here means for all y in the support of Q for any Q that is optimal for
Player II. (A point y is in the support of Q if the Q probability of the interval (y −, y +)
is positive for all  > 0.) So to attempt to find the optimal P, we guess at the set, S, of
“good” points y for Player II and search for a distribution P such that A(P, y) is constant
on S. Such a strategy, P, is called an equalizer strategy on S. The first example shows
what is involved in this.
%===================================================================================%
\subsection{Example 1. Meeting Someone at the Train Station.} A young lady is due to
arrive at a train station at some random time, T, distributed uniformly between noon and
1 PM. She is to wait there until one of her two suitors arrives to pick her up. Each suitor
chooses a time in [0,1] to arrive. If he finds the young lady there, he departs immediately
with her; otherwise, he leaves immediately, disappointed. If either suitor is successful in
meeting the young lady, he receives 1 unit from the other. If they choose the same time
to arrive, there is no payoff. Also, if they both arrive before the young lady arrives, the
payoff is zero. (She takes a taxi at 1 PM.)


Solution: Denote the suitors by I and II, and their strategy spaces by X = [0, 1]
and Y = [0, 1]. Let us find the function A(x, y) that represents I’s expected winnings if I
chooses $x \in X$ and II chooses $y \in Y$ . If x<y, I wins 1 if T <x and loses 1 if x<T <y.
II – 83
The probability of the first is x and the probability of the second is y − x, so A(x, y) is
x − (y − x)=2x − y when x<y. When y<x, a similar analysis shows A(x, y) = x − 2y,
Thus,
A(x, y) =  2x − y if x<y
x − 2y if x>y
0 if x = y.
(1)
This payoff function is not continuous, nor is it upper semicontinuous or lower semicontinuous.
It is symmetric in the players so if it has a value, the value is zero and the players
have the same optimal strategy.

%====================================================================================%
Let us search for an equalizer strategy for Player I and assume it has a density f(x)
on [0,1]. We would have
A(f,y) =  y
0
(2x − y)f(x) dx +
 1
y
(x − 2y)f(x) dx
=
 y
0
(x + y)f(x) dx +
 1
0
(x − 2y)f(x) dx = constant
(2)
Taking a derivative with respect to y yields the equation
2yf(y) +  y
0
f(x) dx − 2
 1
0
f(x) dx = 0 (3)
and taking a second derivative gives
2f(y)+2yf
(y) + f(y) = 0 or f
(y)
f(y) = − 3
2y
. (4)
This differential equation has the simple solution,
log f(y) = −3
2 log(y) + c or f(y) = ky−3/2 (5)
for some constants c and k. Unfortunately,  1
0 y−3/2 dy = ∞, so this cannot be used as a
density on [0,1].

%====================================================================================%
If we think more about the problem, we can see that it cannot be good to come in very
early. There is too little chance that the young lady has arrived. So perhaps the “good”
points are only those from some point a > 0 on. That is, we should look for a density
f(x) on [a, 1] that is an equalizer from a on. So in (2) we replace the integrals from 0 to
integrals from a and assume y>a. The derivative with respect to y gives (3) with the
integrals starting from a rather than 0. And the second derivative is (4) exactly. We have
the same solution (5) but for y>a. This time the resulting f(y) on [a, 1] is a density if
k−1 =
 1
a
x−3/2 dx = −2
 1
a
dx−1/2 = 2(1 − √a)
√a (6)
II – 84
We now need to find a. That may be done by solving equation (3) with the integrals
starting at a.
2yky−3/2 +
 y
a
kx−3/2 dx − 2=2ky−1/2 − 2k(y−1/2 − a−1/2) − 2=2ka−1/2 − 2=0
So ka−1/2 = 1, which implies 1 = 2(1 − √a) or a = 1/4, which in turn implies k = 1/2.
The density
f(x) =  0 if 0 <x< 1/4
(1/2)x−3/2 if 1/4 <x< 1 (7)
is an equalizer for y > 1/4 and is therefore a good candidate for the optimal strategy. We
should still check at points y less than 1/4. For y < 1/4, we have from (2) and (7)
A(f,y) =  1
1/4
(x − 2y)(1/2)x−3/2 dx =
 1
1/4
1
2
√x
dx − 2y = 1
2 − 2y.
So
A(f,y) =  (1 − 4y)/2 for y < 1/4
0 for y > 1/4 (8)
This guarantees I at least 0 no matter what II does. Since II can use the same strategy,
The value of the game is 0 and (7) is an optimal strategy for both players.

%====================================================================================%
\subsection{Example 2. Competing Investors.} Two investors compete to see which of them,
starting with the same initial fortune, can end up with the larger fortune. The rules of
the competition require that they invest only in fair games. That is, they can only invest
non-negative amounts in games whose expected return per unit invested is 1.
Suppose the investors start with 1 unit of fortune each (and we assume money is
infinitely divisible). Thus no matter what they do, their expected fortune at the end is
equal to their initial fortune, 1.
%------------------------------------------------------------------%
Thus the players have the same pure strategy sets. They both choose a distribution
on [0,∞) with mean 1, say Player I chooses F with mean 1, and Player II chooses G with
mean 1. Then Z1 is chosen from F and Z2 is chosen from G independently, and I wins
if Z1 > Z2, II wins if Z2 > Z1 and it is a tie if Z1 = Z2. What distributions should the
investors choose?
The game is symmetric in the players, so the value if it exists is zero, and both players
have the same optimal strategy. Here the strategy spaces are very large, much larger than
in the Euclidean case. But it turns out that the solution is easy to describe. The optimal
strategy for both players is the uniform distribution on the interval (0,2):
F(z) =  z/2 for 0 ≤ z ≤ 2
1 for z > 2
This is a distribution on [0,∞] with mean 1 and so it is an element of the strategy space
of both players. Suppose Player I uses F. Then the probability that I loses is
P(Z1 < Z2) = E[P(Z1 < Z2|Z2)] ≤ E[Z2/2] = (1/2)E[Z2]=1/2.
II – 85
So the probability I wins is at least 1/2. Since the game is symmetric, Player II by using
the same strategy can keep Player I’s probability of winning to at most 1/2.

%====================================================================================%
7.5 Uniform[0,1] Poker Models. The study of two-person Uniform[0,1] poker
models goes back to Borel (1938) and von Neumann (1944). We present these two models
here. In these models, the set of possible “hands” of the players is the interval, [0, 1].
Players I and II are dealt hands x and y respectively in [0, 1] according to a uniform
distribution over the interval [0, 1]. Throughout the play, both players know the value of
their own hand, but not that of the opponent. We assume that x and y are independent
random variables; that is, learning the value of his own hand gives a player no information
about the hand of his opponent.

%====================================================================================%
There follows some rounds of betting in which the players take turns acting. After
the dealing of the hands, all actions that the players take are announced. Except for the
dealing of the hands at the start of the game, this would be a game of perfect information.
Games of this sort, where, after an initial random move giving secret information to the
players, the game is played with no further random moves of nature, are called games of
almost perfect information (See Sorin and Ponssard (1980).
It is convenient to study the action part of games of almost complete information by
what we call the betting tree. This is distinct from the Kuhn tree in that it neglects the
information sets that may arise from the initial distribution of hands. The examples below
illustrate this concept.

%====================================================================================%
The Borel Model: La Relance. Both players contribute an ante of 1 unit into the
pot and receive independent uniform hands on the interval [0, 1]. Player I acts first either
by folding and thus conceding the pot to Player II, or by betting a prescribed amount
β > 0 which he adds to the pot. If Player I bets, then Player II acts either by folding and
thus conceding the pot to Player I, or by calling and adding β to the pot. If Player II calls
the bet of Player I, the hands are compared and the player with the higher hand wins the
entire pot. That is, if x>y then Player I wins the pot; if x<y then Player II wins the
pot. We do not have to consider the case x = y since this occurs with probability 0.
The betting tree is
I
II
bet fold
call fold
+1
−1
±(β +1)
In this diagram, the plus-or-minus sign indicates that the hands are compared, and
the higher hand wins the amount β + 1.
II – 86
It is easy to see that the optimal strategy for Player II must be of the form for some
number b in the interval [0,1]: fold if y<b and call if y>b. The optimal value of b may be
found using the principle of indifference. Player II chooses b to make I indifferent between
betting and folding when I has some hand x<b. If I bets with such an x, he, wins 2 (the
pot) if II has y<b and loses β if II has y>b. His expected winnings are in this case,
2b − β(1 − b). On the other hand, if I folds he wins nothing. (This views the game as a
constant-sum game. It views the money already put into the pot as a sunk cost, and so
the sum of the payoffs of the players is 2 whatever the outcome. This is a minor point but
it is the way most poker players view the pot.) He will be indifferent between betting and
folding if
2b − β(1 − b)=0
from which we conclude
b = β/(2 + β). (1)
Player I’s optimal strategy is not unique, but all of his optimal strategies are of the
form: if x>b, bet; and if x<b, do anything provided the total probability that you fold
is b2. For example, I may fold with his worst hands, i.e. with x<b2, or he may fold with
the best of his hands less than b, i.e. with b − b2 <x<b, or he may, for all 0 <x<b,
simply toss a coin with probability b of heads and fold if the coin comes up heads.
The value of the game may be computed as follows. Suppose Player I folds with any
x<b2 and bets otherwise and suppose Player II folds with y<b. Then the payoff in the
unit square has the values given in the following diagram. The values in the upper right
corner cancel and the rest is easy to evaluate. The value is v(β) = −(β + 1)(1 − b)(b −
b2) + (1 − b2)b − b2, or, recalling b = β/(2 + β),
v(β) = −b2 = − β2
(2 + β)2 . (2)
Thus, the game is in favor of Player II.
0
0
0
0
1
1
y
x
b
b b 2
−1
+1
−(β+1)
−(β+1)
β+1
II – 87
We summarize in
Theorem 7.5. The value of la relance is given by (2). An optimal strategy for Player I
is to bet if x>b − b2 and to fold otherwise, where b is given in (1). An optimal strategy
for Player II is to call if y>b and to fold otherwise.
As an example, suppose β = 2, where the size of the bet is the size of the pot. Then
b = 1/2. An optimal strategy for Player I is to bet if x > 1/4 and fold otherwise; the
optimal strategy of Player II is to call if y > 1/2. The game favors Player II, whose
expected return is 1/4 unit each time the game is played.

%====================================================================================%
If I bets when x<b, he knows he will lose if called, assuming II is using an optimal
strategy. Such a bet is called a bluff. In la relance, it is necessary for I to bluff with
probability b2. Which of the hands below b he chooses to bluff with is immaterial as far as
the value of the game is concerned. However, there is a secondary advantage to bluffing
(betting) with the hands just below b, that is, with the hands from b2 to b. Such a strategy
takes maximum advantage of a mistake the other player may make.
A given strategy σ for a player is called a mistake if there exists an optimal strategy
for the opponent when used against σ gives the opponent an expected payoff better than
the value of the game. In la relance, it is a mistake for Player II to call with some y<b
or to fold with some y>b. If II calls with some y<b, then I can gain from the mistake
most profitably if he bluffs only with his best hands below b.
A strategy is said to be admissible for a player if no other strategy for that player
does better against one strategy of the opponent without doing worse against some other
strategy of the opponent. The rule of betting if and only if x>b2 is the unique admissible
optimal strategy for Player I.

%====================================================================================%
The von Neumann Model. The model of von Neumann differs from the model of
Borel in one small but significant respect. If Player I does not bet, he does not necessarily
lose the pot. Instead the hands are immediately compared and the higher hand wins the
pot. We say Player I checks rather than folds. This provides a better approximation to
real poker and a clearer example of the concept of “bluffing” in poker. The betting tree of
von Neumann’s poker is the same as Borel’s except that the −1 payoff on the right branch
is changed to ±1.
I
II
bet check
call fold
+1
±1
±(β +1)
II – 88
This time it is Player I that has a unique optimal strategy. It is of the form for some
numbers a and b with a<b: bet if x<a or if x>b, and check otherwise. Although there
are many optimal strategies for Player II (and von Neumann finds all of them), one can
show that there is a unique admissible one and it has the simple form: call if y>c for
some number c. It turns out that 0 <a<c<b< 1.
I: | bet | check | bet |
0a b 1
II: | fold | call |
0 c1
The region x<a is the region in which Player I bluffs. It is noteworthy that Player
I must bluff with his worst hands, and not with his moderate hands. It is a mistake for
Player I to do otherwise. Here is a rough explanation of this somewhat counterintuitive
feature. Hands below c may be used for bluffing or checking. For bluffing it doesn’t matter
much which hands are used; one expects to lose them if called. For checking though it
certainly matters; one is better off checking with the better hands.

%====================================================================================%
Let us apply the principle of indifference to find the optimal values of a, b and c. This
will lead to three equations in three unknowns, known as the indifference equations (not
to be confused with difference equations). First, Player II should be indifferent between
folding and calling with a hand y = c. Again we use the gambler’s point of view of the
game as a constant sum game, where winning what is already in the pot is considered as
a bonus. If II folds, she wins zero. If she calls with y = c, she wins (β + 2) if x<a and
loses β if x>b. Equating her expected winnings gives the first indifference equation,
(β + 2)a − β(1 − b)=0. (3)
Second, Player I should be indifferent between checking and betting with x = a. If
he checks with x = a, he wins 2 if y<a, and wins nothing otherwise, for an expected
return of 2a. If he bets, he wins 2 if y<c and loses β if y>c, for an expected return of
2c − β(1 − c). Equating these gives the second indifference equation,
2c − β(1 − c)=2a. (4)

%====================================================================================%
Third, Player I should be indifferent between checking and betting with x = b. If he
checks, he wins 2 if y<b. If he bets, he wins 2 if y<c and wins β + 2 if c<y<b, and
loses β if y>b, for an expected return of 2c + (β + 2)(b − c) − β(1 − b). This gives the
third indifference equation,
2c + (β + 2)(b − c) − β(1 − b)=2b,
which reduces to
2b − c = 1. (5)
II – 89
The optimal values of a, b and c can be found by solving equations (4) (5) and (6) in
terms of β. The solution is
a = β
(β + 1)(β + 4) b = β2 + 4β + 2
(β + 1)(β + 4) c = β(β + 3)
(β + 1)(β + 4). (6)
The value is
v(β) = a = β/((β + 1)(β + 4)). (7)
This game favors Player I. We summarize this in
Theorem 7.6. The value of von Neumann’s poker is given by (7). An optimal strategy
for Player I is to check if a<x<b and to bet otherwise, where a and b are given in (6).
An optimal strategy for Player II is to call if y>c and to fold otherwise, where c is given
in (6).
For pot-limit poker where β = 2, we have a = 1/9, b = 7/9, and c = 5/9, and the
value is v(2) = 1/9.
It is interesting to note that there is an optimal bet size for Player I. It may be found
by setting the derivative of v(β) to zero and solving the resulting equation for β. It is
β = 2. In other words, the optimal bet size is the size of the pot, as in pot-limit poker!
7.6 Exercises.
1. Let X = {−1, 1}, let Y = {..., −2, −1, 0, 1, 2,...} be the set of all integers, and let
A(x, y) = xy.
(a) Show that if we take Y ∗ = Y ∗
F , the set of all finite distributions on Y , then the
value exists, is equal to zero and both players have optimal strategies.
(b) Show that if Y ∗ is taken to be the set of all distributions on Y , then we can’t speak
of the value, because Player II has a strategy, q, for which the expected payoff, A(x, q)
doesn’t exist for any $x \in X$.
2. Simultaneously, Player I chooses x ∈ {x1, x2}, and Player II chooses y ∈ [0, 1]; then
I receives
A(x, y) =  y if x = x1
e−y if x = x2
from II. Find the value and optimal strategies for the players.
3. Player II chooses a point (y1, y2) in the ellipse (y1 − 3)2 + 4(y2 − 2)2 ≤ 4. Simultaneously,
Player I chooses a coordinate k ∈ {1, 2} and receives yk from Player II. Find the
value and optimal strategies for the players.
4. Solve the two games of Example 2. Hint: Use domination to remove some pure
strategies.
II – 90
5. Consider the game with X = [0, 1], Y = [0, 1], and
A(x, y) =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
0 if x = y
−1 if x = 0 and y > 0
+1 if y = 0 and x > 0
−1 if 0 <y<x
+1 if 0 <x<y
Note that A(x, y) is not usc in x for all y nor lsc in y for all x. Show the game does not
have a value.
6. The Greedy Game. Each player can demand from the other as much as desired
between zero and one, but there is a penalty for being too greedy. The player who demands
more than his opponent must pay a fine of b to the other, where b is a fixed number,
0 ≤ b ≤ 1/2. Thus we have the game (X, Y, A) where X = Y = [0, 1], and
A(x, y) = x − y +
 +b if x<y
0 if x = y
−b if x>y
Solve.
7. Find optimal strategies and the value of the following games.
(a) X = Y = [0, 1] and A(x, y) =  (x − y)2 if x ≤ y
2(x − y)2 if x ≥ y. (Underestimation is the more
serious error of Player II.)
(b) X = Y = [0, 1] and A(x, y) = xe−y + (1 − x)y.
8. Hide and Seek in a Compact, Convex Set. (a) Let S be the triangle in the
plane with vertices (−1, 0), (1, 0), and (0, 2). Player I chooses a point x in S in which to
hide, and Player II chooses a point y in S to seek. The payoff to Player I is the square
of Euclidean distance between x and y. Thus, X = S, Y = S, and A(x, y) = x − y2.
Solve.
(b) See if you can formulate a procedure for solving the above game of Hide and Seek
if S is an arbitrary compact, convex set in Rn.
9. The Wallet Game. Two players each put a random amount with mean one into
their wallets. The player whose wallet contains the smaller amount wins the larger amount
from the opponent.
Carroll, Jones and Rykken (2001) show that this game does not have a value. But
suppose we restrict the players to putting at most some amount b in their wallets. Here is
the game:
Player I, resp. Player II, chooses a distribution F, resp. G, on the interval [0, b] with
mean 1, where b > 1. Then independent random variables, X from F and Y from G, are
II – 91
chosen. If X<Y , Player I wins Y from Player II. If X>Y , Player II wins X from Player
I, and if X = Y , there is no payoff. So the payoff function is
A(F, G) = E(Y I(X<Y ) − XI(X>Y ))
= E((Y + X)I(X<Y )) − 1 + E(XI(X = Y )). (1)
The game is symmetric, so if the value exists, the value is zero, and the players have the
same optimal strategies. Find an optimal strategy for the players. Hint: Search among
distributions F having a density f on the interval (a, b) for some a < 1. Note that the last
term on the right of Equation (1) disappears for such distributions.
10. The Multiplication Game. (See Kent Morrison (2010).) Players I and II
simutaneously select positive numbers x and y. Player I wins +1 if the product xy, written
in decimal form has initial significant digit 1, 2 or 3. Thus, the pure strategy spaces are
X = Y = (0,∞) and the payoff function is
A(x, y) =  +1 if the initial significant digit is 1, 2 or 3
0 otherwise.
Solve.
Hint:(1) First note that both players may restrict their pure strategy sets to X =
Y = [1, 10). so that
A(x, y) = I{1 ≤ xy < 4 or 10 ≤ xy < 40}.
(2) Take logs to the base 10. Let u = log10(x) and v = log10(y). Now, players I and
II choose u and v in [0,1) with payoff
B(u, v) = I{0 ≤ u + v<c or 1 ≤ u + v < 1 + c}
where c = log10(4) = .60206 .... Solve the game in this form and translate back to the
original game.
11. Suppose, in La Relance, that when Player I checks, Player II is given a choice
between checking in which case there is no payoff, and calling in which case the hands are
compared and the higher hand wins the antes.
(a) Draw the betting tree.
(b) Assume optimal strategies of the following form. I checks if and only if 0 <a<
x<b< 1 for some a and b. If I bets, then II calls iff y>c, and if Player I checks, Player
II calls iff y>d, where a ≤ c ≤ b and a ≤ d ≤ b. Find the indifference equations.
(c) Solve the equations when β = 2, and find the value in this case. Which player has
the advantage?
12. Last Round Betting. Here is a game that occurs in the last round of blackjack
or baccarat tournaments, and also in the television game show, Final Jeopardy. For the
general game, see Ferguson and Melolidakis (1997).
II – 92
In the last round of betting in a contest to see who can end up with the most money,
Player I starts with $70 and Player II starts with $100. Simultaneously, Player I must
choose an amount to bet between $0 and $70 and Player II must choose an amount between
$0 and $100. Then the players independently play games with probability .6 of winning
the bet and .4 of losing it. The player who has the most money at the end wins a big prize.
If they end up with the same amount of money, they share the prize.
%====================================================================================%
We may set this up as a game (X, Y, A), with X = [0, 0.7], Y = [0, 1.0], measured in
units of $100, and assuming money is infinitely divisible. We assume the payoff, A(x, y),
is the probability that Player I wins the game plus one-half the probabiity of a tie, when
I bets x and II bets y. The probability that both players win their bets is .6 ∗ .6 = .36,
the probability that both players lose their bets is .4 ∗ .4 = .16, and the probability that I
wins his bet and II loses her bet is .6 ∗ .4 = .24. Therefore,
P(I wins) = .36 I(.7 + x > 1 + y) + .24 I(.7 + x > 1 − y) + .16 I(.7 − x > 1 − y)
= .36 I(x − y>.3) + .24 I(x + y>.3) + .16 I(y − x>.3)
P(a tie) = .36 I(.7 + x =1+ y) + .24 I(.7 + x = 1 − y) + .16 I(.7 − x = 1 − y)
= .36 I(x − y = .3) + .24 I(x + y = .3) + .16 I(y − x = .3)
where I(·) represents the indicator function. This gives
A(x, y) = P(I wins) + 1
2
P(a tie) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
.60 if y<x − .3
.40 if y>x + .3
.00 if y + x<.3
.24 if y>x − .3,y<x + .3, y + x>.3
.42 if 0 < y = x − .3
.32 if 0 < x = y − .3
.12 if x + y = .3,x> 0,y> 0
.30 if x = .3, y = 0
.20 if x = 0, y = .3
Find the value of the game and optimal strategies for both players. (Hint: Both players
have an optimal strategy that give probability to only two points.)

