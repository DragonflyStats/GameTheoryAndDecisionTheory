6. Recursive and Stochastic Games
6.1 Matrix Games with Games as Components. We consider now matrix games
in which the outcome of a particular choice of pure strategies of the players may be that
the players have to play another game. Let us take a simple example.
Let G1 and G2 denote 2 × 2 games with matrices
G1 =
 0 3
2 –1
and G2 =
 0 1
4 3
and let G denote the 2 × 2 game whose matrix is represented by
G =
 G1 4
5 G2

.
The game G is played in the usual manner with Player I choosing a row and Player II
choosing a column. If the entry in the chosen row and column is a number, II pays I that
amount and the game is over. If I chooses row 1 and II chooses column 1, then the game
G1 is played. If I chooses row 2 and II chooses column 2, then G2 is played.
We may analyze the game G by first analyzing G1 and G2.
G1 : Optimal for I is (1/2,1/2)
Optimal for II is (2/3,1/3)
Val(G1)=1.
G2 : Optimal for I is (0,1)
Optimal for II is (0,1)
Val(G2)=3.
If after playing the game G the players end up playing G1, then they can expect a payoff
of the value of G1, namely 1, on the average. If the players end up playing G2, they can
expect an average payoff of the value of G2, namely 3. Therefore, the game G can be
considered equivalent to the game with matrix
 1 4
5 3
G :
Optimal for I is (2/5,3/5)
Optimal for II is (1/5,4/5)
Val(G)=17/5.
This method of solving the game G may be summarized as follows. If the matrix of
a game G has other games as components, the solution of G is the solution of the game
whose matrix is obtained by replacing each game in the matrix of G by its value.
II – 61
Decomposition. This example may be written as a 4×4 matrix game. The four pure
strategies of Player I may be denoted {(1, 1),(1, 2),(2, 1),(1, 2)}, where (i, j) represents:
use row i in G, and if this results in Gi being played use row j. A similar notation may
be used for Player II. The 4 × 4 game matrix becomes
G =
⎛
⎜⎝
0 3
2 –1
4 4
4 4
5 5
5 5
0 1
4 3
⎞
⎟⎠
We can solve this game by the methods of Chapter 4.
Conversely, suppose we are given a game G and suppose after some rearrangement of
the rows and of the columns the matrix may be decomposed into the form
G =
 G11 G12
G21 G22
where G11 and G22 are arbitrary matrices and G12 and G21 are constant matrices. (A
constant matrix has the same numerical value for all of its entries.) Then we can solve
G by the above method, pretending that as the first move the players choose a row and
column from the 2 × 2 decomposed matrix. See Exercise 1(b).
6.2 Multistage Games. Of course, a game that is the component of some matrix
game may itself have other games as components, in which case one has to iterate the
above method to obtain the solution. This works if there are a finite number of stages.
Example 1. The Inspection Game. (M. Dresher (1962)) Player II must try to perform
some forbidden action in one of the next n time periods. Player I is allowed to inspect II
secretly just once in the next n time periods. If II acts while I is inspecting, II loses 1 unit
to I. If I is not inspecting when II acts, the payoff is zero.
Let Gn denote this game. We obtain the iterative equations
Gn =

act wait
inspect 1 0
wait 0 Gn−1

for n = 2, 3,...
with boundary condition G1 = (1). We may solve iteratively.
Val(G1)=1
Val(G2) = Val  1 0
0 1
= 1/2
Val(G3) = Val  1 0
0 1/2

= 1/3
.
.
.
Val(Gn) = Val  1 0
0 1/(n − 1)
= 1/n
II – 62
since inductively, Val(Gn) = 1
n−1 /(1 + 1
n−1 )=1/n. The optimal strategy in the game Gn
for both players is (1/n,(n − 1)/n). For other games of this sort, see the book by Garnaev
(2000).
Example 2. Guess it! (Rufus Isaacs (1955); see also Martin Gardner (1978), p. 40.) As
a more complex example of a multistage game, consider the following game loosely related
to the game Cluedo. From a deck with m + n + 1 distinct cards, m cards are dealt to
Player I, n cards are dealt to Player II, and the remaining card, called the “target card”,
is placed face down on the table. Players knows their own cards but not those of their
opponent. The objective is to guess correctly the target card. Players alternate moves,
with Player I starting. At each move, a player may either
(1) guess at the target card, in which case the game ends, with the winner being the player
who guessed if the guess is correct, and his opponent if the guess is incorrect, or
(2) ask if the other player holds a certain card. If the other player has the card, that card
must be shown and is removed from play.
With a deck of say 11 cards and each player receiving 5 cards, this is a nice playable
game that illustrates need for bluffing in a clear way. If a player asks about a card that is
in his own hand, he knows what the answer will be. We call such a play a bluff . If a player
asks about a card not in his hand, we say he is honest. If a player is always honest and
the card he asks about is the target card, the other player will know that the requested
card is the target card and so will win. Thus a player must bluff occasionally. Bluffing
may also lure the opponent into a wrong guess at the target card.
Let us denote this game with Player I to move by Gm,n. The game Gm,0 is easy to
play. Player I can win immediately. Since his opponent has no cards, he can tell what the
target card is. Similarly, the game G0,n is easy to solve. If Player I does not make a guess
immediately, his opponent will win on the next move. However, his probability of guessing
correctly is only 1/(n + 1). Valuing 1 for a win and 0 for a loss from Player I’s viewpoint,
the value of the game is just the probability I wins under optimal play. We have
Val(Gm,0) = 1 for all m ≥ 0, and Val(G0,n) = 1
n + 1
for all n ≥ 0. (1)
If Player I asks for a card that Player II has, that card is removed from play and it is
Player II’s turn to move, holding n − 1 cards to her opponent’s m cards. This is exactly
the game Gn−1,m but with Player II to move. We denote this game by Gn−1,m. Since the
probability that Player I wins is one minus the probability that Player II wins, we have
Val(Gn,m)=1 − Val(Gn,m) for all m and n. (2)
Suppose Player I asks for a card that Player II does not have. Player II must immediately
decide whether or not Player I was bluffing. If she decides Player I was honest, she
will announce the card Player I asked for as her guess at the target card, and win if she
was right and lose if she was wrong. If she decides Player I was bluffing and she is wrong,
Player I will win on his turn. If she is correct, the card Player I asked for is removed from
his hand, and the game played next is Gn,m−1.
II – 63
Using such considerations, we may write the game as a multistage game in which
a stage consists of three pure strategies for Player I (honest, bluff, guess) and two pure
strategies for Player II (ignore the asked card, call the bluff by guessing the asked card).
The game matrix becomes, for m ≥ 1 and n ≥ 1,
Gm,n =
⎛
⎝
ignore call
honest n
n+1Gn−1,m + 1
n+1
n
n+1Gn−1,m
bluff Gn,m−1 1
guess 1
n+1
1
n+1
⎞
⎠ (3)
This assumes that if Player I asks honestly, he chooses among the n + 1 unknown
cards with probability 1/(n +1) each; also if he bluffs, he chooses among his m cards with
probability 1/m each. That this may be done follows from the invariance considerations
of Section 3.6.
As an example, the upper left entry of the matrix is found as follows. With probability
n/(n + 1), Player I asks a card that is in Player II’s hand and the game becomes Gn−1,m;
with probability 1/(n + 1), Player I asks the target card, Player II ignors it and Player I
wins on his next turn, i.e. gets 1. The upper right entry is similar, except this time if the
asked card is the target card, Player II guesses it and Player I gets 0.
It is reasonable to assume that if m ≥ 1 and n ≥ 1, Player I should not guess,
because the probability of winning is too small. In fact if m ≥ 1 and n ≥ 1, there is a
strategy for Player I that dominates guessing, so that the last row of the matrix may be
deleted. This strategy is: On the first move, ask any of the m + n + 1 cards with equal
probability 1/(m + n + 1) (i.e. use row 1 with probability (n + 1)/(m + n + 1) and row
2 with probability m/(m + n + 1)), and if Player II doesn’t guess at her turn, then guess
at the next turn. We must show that Player I wins with probability at least 1/(n + 1)
whether or not Player II guesses at her next turn. If Player II guesses, her probability of
win is exactly 1/(m + 1) whether or not the asked card is one of hers. So Player I’s win
probability is m/(m + 1) ≥ 1/2 ≥ 1/(n + 1). If Player II does not guess, then at Player
I’s next turn, Player II has at most n cards (she may have n − 1) so again Player I’s win
probability is at least 1/(n + 1).
So the third row may be removed in (3) and the games reduce to
Gm,n =

ignore call
honest n
n+1Gn−1,m + 1
n+1
n
n+1Gn−1,m
bluff Gn,m−1 1

(4)
for m ≥ 1 and n ≥ 1. These 2 by 2 games are easily solved recursively, using the boundary
conditions (1). One can find the value and optimal strategies of Gm,n after one finds the
values of Gn,m−1 and Gn−1,m and uses (2). For example, the game G1,1 reduces to the
game with matrix  3/4 1/4
0 1
. The value of this game is 1/2, an optimal strategy for
II – 64
Player I is (2/3,1/3) (i.e. bluff with probability 1/3), and the optimal strategy of player
II is (1/2,1/2).
One can also show that for all m ≥ 1 and n ≥ 1 these games do not have saddle points.
In fact, one can show more: that Val(Gm,n) is nondecreasing in m and nonincreasing in n.
(The more cards you have in your hand, the better.) Let Vm,n = Val(Gm,n). Then using
Val  a b
c d
= (ad − bc)/(a + d − b − c), we have after some algebra
Vm,n = Val  n
n+1 (1 − Vn−1,m) + 1
n+1
n
n+1 (1 − Vn−1,m)
(1 − Vn,m−1) 1
= 1 + n(1 − Vn−1,m)Vn,m−1
1+(n + 1)Vn,m−1
.
for m ≥ 1 and n ≥ 1. This provides a simple direct way to compute the values recursively.
The following table gives the computed values as well as the optimal strategies for the
players for small values of m and n.
m\n 123456
0.5000 0.5000 0.4000 0.3750 0.3333 0.3125
1 0.3333 0.2500 0.2000 0.1667 0.1429 0.1250
0.5000 0.5000 0.4000 0.3750 0.3333 0.3125
0.6667 0.5556 0.5111 0.4500 0.4225 0.3871
2 0.5000 0.3333 0.2667 0.2143 0.1818 0.1563
0.3333 0.3333 0.2889 0.2500 0.2301 0.2055
0.6875 0.6250 0.5476 0.5126 0.4667 0.4411
3 0.5000 0.3750 0.2857 0.2361 0.1967 0.1701
0.3750 0.3250 0.2762 0.2466 0.2167 0.1984
0.7333 0.6469 0.5966 0.5431 0.5121 0.4749
4 0.5556 0.3947 0.3134 0.2511 0.2122 0.1806
0.3333 0.3092 0.2634 0.2342 0.2118 0.1899
0.7500 0.6809 0.6189 0.5810 0.5389 0.5112
5 0.5714 0.4255 0.3278 0.2691 0.2229 0.1917
0.3333 0.2908 0.2566 0.2284 0.2062 0.1885
0.7714 0.6972 0.6482 0.6024 0.5704 0.5353
6 0.6000 0.4410 0.3488 0.2808 0.2362 0.2003
0.3143 0.2834 0.2461 0.2236 0.2028 0.1854
Table of values and optimal strategies of Gm,n for 1 ≤ m, n ≤ 6. The top number in each
box is the value, the middle number is the probability with which Player I should bluff,
and the bottom number is the probability with which Player II should call the asked card.
II – 65
6.3 Recursive Games. -Optimal Strategies. In some games with games as
components, it may happen that the original game comes up again. Such games are called
recursive. A simple example is
G =
 G 1
1 0
This is an infinite game. If the players always play the first row and first column, the game
will be played forever. No matter how unlikely such a possibility is, the mathematical
definition is not complete until we say what the payoff is if G is played forever. Let us say
that II pays I Q units if they both choose their first pure strategy forever, and write
G =
 G 1
1 0
, Q.
We are not automatically assured the existence of a value or the existence of optimal
strategies in infinite games. However, it is easy to see that the value of G exists and is
equal to 1 no matter what the value of the number Q is. The analysis can be made as
follows.
II can restrict her losses to at most 1 by choosing the second column. If Q ≥ 1, I can
guarantee winning at least 1 by playing his first row forever. But if Q < 1, this won’t work.
It turns out that an optimal strategy for I, guaranteeing him at least 1, does not exist in
this case. However, for any  > 0 there is a strategy for I that guarantees him an average
gain of at least 1 − . Such a strategy, that guarantees a player an average payoff within
 of the value, is called -optimal. In this case, the strategy that continually uses the
mixed strategy (1 − , ) (top row with probability 1 −  and bottom row with probability
) is -optimal for I. The use of such a strategy by I insures that he will eventually choose
row 2, so that the payoff is bound to be 0 or 1 and never Q. The best that Player II can
do against this strategy is to choose column 2 immediately, hoping that I chooses row 2.
The expected payoff would then be 1 · (1 − )+0 ·  = 1 − .
In summary, for the game G above, the value is 1; Player II has an optimal strategy,
namely column 2; If Q ≥ 1, the first row forever is optimal for I; if Q < 1, there is no
optimal strategy for I, but the strategy (1 − , ) forever is -optimal for I.
Consider now the game
G0 =
 G0 5
1 0
, Q.
For this game, the value depends on Q. If Q ≥ 1, the first row forever is optimal for I,
and the value is Q if 1 ≤ Q ≤ 5, and the value is 5 if Q ≥ 5. For Q < 1, the value is 1;
however, in contrast to the game G above, I has an optimal strategy for the game G0, for
example (1/2, 1/2) forever. II’s optimal strategy is the first column forever if Q < 5, the
second column if Q > 5 and anything if Q = 5.
In analogy to what we did for games with games as components, we might attempt
to find the value v of such a game by replacing G0 by v in the matrix and solving the
II – 66
equation
v = Val  v 5
1 0
for v. Here there are many solutions to this equation. The set of all solutions to this
equation is the set of numbers v in the interval 1 ≤ v ≤ 5. (Check this!)
This illustrates a general result that the equation, given by equating v to the value of
the game obtained by replacing the game in the matrix by v, always has a solution equal
to the value of the game. It may have more solutions but the value of the game is that
solution that is closest to Q. For more information on these points, consult the papers of
Everett (1957) and of Milnor and Shapley (1957).
Example 3. Let
G =
⎛
⎝
G 1 0
1 0 G
0 G 1
⎞
⎠ , Q.
Then, if the value of G is v,
v = Val
⎛
⎝
v 1 0
1 0 v
0 v 1
⎞
⎠ = 1 + v
3 .
This equation has a unique solution, v = 1/2. This must be the value for all Q. The
strategy (1/3,1/3,1/3) forever is optimal for both players.
Example 4. The basic game of Dare is played as follows. Player I, the leader, and Player
II, the challenger, simultaneously “pass” or “dare”. If both pass, the payoff is zero (and
the game is over). If I passes and II dares, I wins 1. If I dares and II passes, I wins 3. If
both dare, the basic game is played over with the roles of the players reversed (the leader
becomes the challenger and vice versa). If the players keep daring forever, let the payoff
be zero. We might write
G =

pass dare
pass 0 1
dare 3 −GT

where −GT represents the game with the roles of the players reversed. (Its matrix is the
negative of the transpose of the matrix G.) The value of −GT is the negative of the value
of G.
If v represents the value of G, then v ≥ 0 because of the top row. Therefore the matrix
for G with −GT replaced by −v does not have a saddle point, and we have
v = Val  0 1
3 −v

= 3
4 + v
.
This gives the quadratic equation, v2 + 4v − 3 = 0. The only nonnegative solution is
v = √7 − 2 = .64575 ···. The optimal strategy for I is ((5 − √7)/3, (
√7 − 2)/3) and the
optimal strategy for II is (3 − √7,
√
7 − 2).
II – 67
Example 5. Consider the following three related games.
G1 =
 G2 0
0 G3

G2 =
 G1 1
1 0
G3 =
 G1 2
2 0
and suppose the payoff if the games are played forever is Q. Let us attempt to solve these
games. Let v1 = Val(G1), v2 = Val(G2), and v3 = Val(G3). Player I can guarantee that
v1 > 0, v2 > 0 and v3 > 0 by playing (1/2, 1/2) forever. In addition, v2 ≤ 1 and v3 ≤ 2,
which implies v1 < 1. Therefore none of the games has a saddle point and we may write
v1 = v2v3
v2 + v3
, v2 = 1
2 − v1
, v3 = 4
4 − v1
.
Substituting the latter two equations into the former, we obtain
v1
2 − v1
+
4v1
4 − v1
= 4
(2 − v1)(4 − v1)
5v2
1 − 12v1 +4=0
(5v1 − 2)(v1 − 2) = 0
Since 0 < v1 < 1, this implies that v1 = 2/5. Hence
Game value opt. for I= opt. for II
G1 2/5 (16/25, 9/25)
G2 5/8 (5/8, 3/8)
G3 10/9 (5/9, 4/9)
independent of the value of Q.
6.4 Stochastic Movement Among Games. We may generalize the notion of a
recursive game by allowing the choice of the next game played to depend not only upon the
pure strategy choices of the players, but also upon chance. Let G1,...,Gn be games and
let p1,...,pn be probabilities that sum to one. We use the notation, p1G1 +···+pnGn, to
denote the situation where the game to be played next is chosen at random, with game Gi
being chosen with probability pi, i = 1,...,n. Since, for a given number z, the 1×1 matrix
(z) denotes the trivial game in which II pays I z, we may, for example, use 1
2G1 + 1
2 (3) to
represent the situation where G1 is played if a fair coin comes up heads, and II pays I 3
otherwise.
Example 6. Let G1 and G2 be related as follows.
G1 =
 1
2G2 + 1
2 (0) 1
2 0
G2 =
 2
3G1 + 1
3 (−2) 0
0 −1

The game must eventually end (with probability 1). In fact, the players could not play
forever even if they wanted to. Even if they choose the first row and first column forever,
II – 68
eventually the game would end with a payoff of 0 or −2. Thus we do not need to specify
any payoff if play continues forever. To solve, let vi = Val(Gi) for i = 1, 2. Then 0 ≤ v1 ≤ 1
and −1 ≤ v2 ≤ 0, so neither game has a saddle point. Hence,
v1 = Val  1
2 v2 1
2 0
= 4
6 − v2
and
v2 = Val  2
3 v1 − 2
3 0
0 −1

= −2(1 − v1)
5 − 2v1
Thus
v1 = 4
6 + 2(1−v1)
5−2v1
= 2(5 − 2v1)
16 − 7v1
.
This leads to the quadratic equation, 7v2
1−20v1+10 = 0, with solution, v1 = (10−
√30)/7 =
.646 ···. We may substitute back into the equation for v2 to find v2 = −(2√30 − 10)/5 =
−.191 ···. From these values one can easily find the optimal strategies for the two games.
Example 7. A coin with probability 2/3 of heads is tossed. Both players must guess
whether the coin will land heads or tails. If I is right and II is wrong, I wins 1 if the coin
is heads and 4 if the coin is tails and the game is over. If I is wrong and II is right, there
is no payoff and the game is over. If both players are right, the game is played over. But
if both players are wrong, the game is played over with the roles of the players reversed.
If the game never ends, the payoff is Q.
If we denote this game by G, then
G =
 2
3G + 1
3 (−GT ) 2
3 (1) + 1
3 (0)
2
3 (0) + 1
3 (4) 2
3 (−GT ) + 1
3G

If we let its value be denoted by v, then
v = Val  1
3 v 2
3 4
3 −1
3 v

If v ≥ 2, then there is a saddle at the upper right corner with v = 2/3. This contradiction
shows that v < 2 and there is no saddle. Therefore,
v = 8 + v2
18
or v2 − 18v +8=0.
This has a unique solution less than two,
v = 9 − √
73 = .456 ···
from which we may calculate the optimal strategy for I:
(
13 − √73
6 ,
√
73 − 7
6 )=(.743 ··· , .256 ···)
II – 69
and the optimal strategy for II:
(
11 − √
73
6 ,
√
73 − 5
6 )=(.409 ··· , .591 ···).
The value and optimal strategies are independent of Q.
6.5 Stochastic Games. If to the features of the games of the previous section is
added the possibility of a payoff at each stage until the game ends, the game is called
a Stochastic Game. This seems to be the proper level of generality for theoretical
treatment of multistage games. It is an area of intense contemporary research. See for
example the books of Filar and Vrieze (1997) and Maitra and Sudderth (1996). Stochastic
games were introduced by Shapley in (1953) in a beautiful paper that has been reprinted
in Raghavan et al. (1991), and more recently in Kuhn (1997). In this section, we present
Shapley’s main result.
A Stochastic Game, G, consists of a finite set of positions or states, {1, 2,...,N}, one
of which is specified as the starting position. We denote by G(k) the game in which k is the
starting position. Associated with each state, k, is a matrix game, A(k) = 
a(k)
ij 

. If the
stochastic game is in state k, the players simultaneously choose a row and column of A(k),
say i and j. As a result, two things happen. First, Player I wins the amount a(k)
ij from
Player II. Second, with probabilities that depend on i, j and k, the game either stops, or
it moves to another state (possibly the same one). The probability that the game stops is
denoted by s
(k)
ij , and the probability that the next state is  is denoted by P(k)
ij (), where
s
(k)
ij +

N
=1
P(k)
ij () = 1 (5)
for all i, j and k.
The payoffs accumulate throughout the game until it stops. To make sure the game
eventually stops, we make the assumption that all the stopping probabilities are positive.
Let s denote the smallest of these probabilities.
s = min
i,j,k
s
(k)
ij > 0 (6)
Under this assumption, the probability is one that the game ends in a finite number of
moves. This assumption also makes the expected accumulated payoff finite no matter how
the game is played, since if M denotes the largest of the absolute values of the payoffs,
M = maxi,j,k |a(k)
ij |, then the total expected payoff to either player is bounded by
M + (1 − s)M + (1 − s)
2M + ··· = M/s. (7)
Player I wishes to maximize the total accumulated payoff and Player II to minimize
it. We use a modification of the notation of the previous section to describe this game.
II – 70
G(k) =

a
(k)
ij +

N
=1
P(k)
ij ()G()

. (8)
Note that the probabilities in each component of this matrix sum to less than one. It is
understood that with the remaining probability, s
(k)
ij , the game ends. It should be noted
that in contrast to the previous section, a payoff does not end the game. After a payoff is
made, it is then decided at random whether the game ends and, if not, which state should
be played next.
Since no upper bound can be placed on the length of the game, this is an infinite
game. A strategy for a player must specify for every n how to choose an action if the game
reaches stage n. In general, theory does not guarantee a value. Moreover, the choice of
what to do at stage n may depend on what happened at all previous stages, so the space
of possible strategies is extremely complex.
Nevertheless, in stochastic games, the value and optimal strategies for the players
exist for every starting position. Moreover, optimal strategies exist that have a very simple
form. Strategies that prescribe for a player a probability distribution over his choices that
depends only on the game, Gk, being played and not on the stage n or past history are
called stationary strategies. The following theorem states that there exist stationary
optimal strategies.
Theorem 1. (Shapley (1952)) Each game G(k) has a value, v(k). These values are the
unique solution of the set of equations,
v(k) = Val 
a(k)
ij +

N
=1
P(k)
ij () v()

for k = 1,...,N. (9)
Each player has a stationary optimal strategy that in state k uses the optimal mixed
strategy for the game with matrix
A(k)
(v) = 
a(k)
ij +

N
=1
P(k)
ij () v()

(10)
where v represents the vector of values, v = (v(1),...,v(N)).
In equations (9), we see the same principle as in the earlier sections: the value of a
game is the value of the matrix game (8) with the games replaced by their values. A proof
of this theorem may be found in Appendix 2.
Example 8. As a very simple example, consider the following stochastic game with
one state, call it G.
G =
 1 + (3/5)G 3 + (1/5)G
1 + (4/5)G 2 + (2/5)G

II – 71
From Player II’s viewpoint, column 1 is better than column 2 in terms of immediate payoff,
but column 2 is more likely to end the game sooner than column 1, so that it should entail
smaller future payoffs. Which column should she choose?
Assume that all strategies are active, i.e. that the game does not have a saddle point.
We must check when we are finished to see if the assumption was correct. Then
v = Val  1 + (3/5)v 3 + (1/5)v
1 + (4/5)v 2 + (2/5)v

= (1 + (4/5)v)(3 + (1/5)v) − (1 + (3/5)v)(2 + (2/5)v)
1 + (4/5)v + 3 + (1/5)v − 1 − (3/5)v − 2 − (2/5)v
=1+ v − (2/25)v2
This leads to
(2/25)v2 = 1.
Solving this quadratic equation gives two possible solutions v = ±
25/2 = ±(5/2)√
2.
Since the value is obviously positive, we must use the plus sign. This is v = (5/2)√
2 =
3.535. If we put this value into the matrix above, it becomes
 1 + (3/2)√2 3 + (1/2)√2
1+2√2 2+ √2

The optimal strategy for Player I in this matrix is p = (√
2 − 1, 2 − √
2) = (.414, .586),
and the optimal strategy for Player II is q = (1 − √2/2,
√2/2) = (.293, .707). Since these
are probability vectors, our assumption is correct and these are the optimal strategies, and
v = (5/2)√2 is the value of the stochastic game.
6.6 Approximating the solution. For a general stochastic game with many states,
equations (9) become a rather complex system of simultaneous nonlinear equations. We
cannot hope to solve such systems in general. However, there is a simple iterative method
of approximating the solution. This is based on Shapley’s proof of Theorem 1, and is called
Shapley iteration.
First we make a guess at the solution, call it v0 = (v0(1),...,v0(N)). Any guess will
do. We may use all zero’s as the initial guess, v0 = 0 = (0,..., 0). Then given vn, we
define inductively, vn+1, by the equations,
vn+1(k) = Val 
a
(k)
ij +

N
=1
P(k)
ij () vn()

for k = 1,...,N. (11)
With v0 = 0, the vn(k) have an easily understood interpretation. vn(k) is the value of the
stochastic game starting in state k if there is forced stopping if the game reaches stage n.
In particular, v1(k) = Val(Ak) for all k.
The proof of Theorem 1 shows that vn(k) converges to the true value, v(k), of the
stochastic game starting at k. Two useful facts should be noted. First, the convergence
II – 72
is at an exponential rate: the maximum error goes down at least as fast as (1 − s)n.
(See Corollary 1 of Appendix 2.) Second, the maximum error at stage n + 1 is at most
the maximum change from stage n to n + 1 multiplied by (1 − s)/s. (See Corollary 2 of
Appendix 2.)
Let us take an example of a stochastic game with two positions. The corresponding
games G(1) and G(2), are related as follows.
G(1) =
 4 + .3G(1) 0 + .4G(2)
1 + .4G(2) 3 + .5G(1)
G(2) =
 0 + .5G(1) −5
−4 1+ .5G(2)
Using v0 = (0, 0) as the initial guess, we find v1 = (2, −2), since
v1(1) = Val  4 0
1 3
= 2 v1(2) = Val  0 −5
−4 1
= −2.
The next iteration gives
v2(1) = Val  4.6 −.8
.2 4
= 2.0174 v2(2) = Val  1 −5
−4 0
= −2.
Continuing, we find
v3(1) = 2.0210 v3(2) = −1.9983
v4(1) = 2.0220 v4(2) = −1.9977
v5(1) = 2.0224 v5(2) = −1.9974
v6(1) = 2.0225 v6(2) = −1.9974
The smallest stopping probability is .5, so the rate of convergence is at least (.5)n and the
maximum error of v6 is at most .0002.
The optimal strategies using v6 are easily found. For game G(1), the optimal strategies
are p(1) = (.4134, .5866) for Player I and q(1) = (.5219, .4718) for Player II. For game G(2),
the optimal strategies are p(2) = (.3996, .6004) for Player I and q(2) = (.4995, .5005) for
Player II.
6.7 Exercises
1.(a) Solve the system of games
G =
 0 G1
G2 G3

G1 =
 4 3
1 2
G2 =
 0 6
5 1
G3 =
 0 −2
−2 0
.
(b). Solve the games with matrices
(b1) (b2)
⎛
⎜⎝
0602
0305
5020
1040
⎞
⎟⎠
⎛
⎜⎜⎜⎝
31522
13522
44122
11163
11147
⎞
⎟⎟⎟⎠
II – 73
2. The Inspection Game. Let Gm,n denote the inspection game in which I is
allowed m inspections in the n time periods. (Thus, for 1 ≤ n ≤ m, Val(Gm,n) = 1, while
for n ≥ 1, Val(G0,n) = 0.) Find the iterative structure of the games and solve.
3. A Game of Endurance. II must count from n down to zero by subtracting
either one or two at each stage. I must guess at each stage whether II is going to subtract
one or two. If I ever guesses incorrectly at any stage, the game is over and there is no
payoff. Otherwise, if I guesses correctly at each stage, he wins 1 from II. Let Gn denote
this game, and use the initial conditions G0 = (1) and G1 = (1). Find the recursive
structure of the games and solve. (In the solution, you may use the notation Fn to denote
the Fibonacci sequence, 1, 1, 2, 3, 5, 8, 13,... , with definition F0 = 1, F1 = 1, and for
n ≥ 2, Fn = Fn−1 + Fn−2.)
4. Solve the sequence of games, G0, G1,..., where
G0 =
 3 2
1 G1

,...,Gn =
 n + 3 n + 2
n + 1 Gn+1
,...
Assume that if play continues forever, the payoff is zero.
5. (a) In the game “Guess it!”, G1,n, with m = 1 and arbitrary n, show that Player
I’s optimal strategy if to bluff with probability 1/(n + 2).
(b) Show that Player II’s optimal strategy in G1,n is to call the asked card with
probability V1,n, the value of G1,n.
6. Recursive Games. (a) Solve the game G =
 G 2
0 1
, Q.
(b) Solve the game G =
⎛
⎝
G 1 1
1 0 G
1 G 0
⎞
⎠ , Q.
7. Consider the following three related games.
G1 =
 G2 1
1 0
G2 =
 G3 0
0 2
G3 =
 G1 1
1 0
and suppose the payoff is Q if the games are played forever. Solve.
8. Consider the following three related games.
G1 =
⎛
⎝
G1 G2 G3
G2 G3 G1
G3 G1 G2
⎞
⎠ G2 =
 G1 0
0 2
G3 =
 G2 1
1 0
and suppose the payoff is Q if the games are played forever. Solve.
II – 74
9. There is one point to go in the match. The player that wins the last point while
serving wins the match. The server has two strategies, high and low. The receiver has
two strategies, near and far. The probability the server wins the point is given in the
accompanying table.
near far
high .8 .5
low .6 .7
If the server misses the point, the roles of the players are interchanged and the win probabilities
for given pure strategies are the same for the new server. Find optimal strategies
for server and receiver, and find the probability the server wins the match.
10. Player I tosses a coin with probability p of heads. For each k = 1, 2,..., if I tosses
k heads in a row he may stop and challenge II to toss the same number of heads; then II
tosses the coin and wins if and only if he tosses k heads in a row. If I tosses tails before
challenging II, then the game is repeated with the roles of the players reversed. If neither
player ever challenges, the game is a draw.
(a) Solve when p = 1/2.
(b) For arbitrary p, what are the optimal strategies of the players? Find the limit as p → 1
of the probability that I wins.
11. Solve the following stochastic game.
G =
 4 1 + (1/3)G
0 1 + (2/3)G

.
12. Consider the following stochastic game with two positions.
G(1) =
 2 2 + (1/2)G(2)
0 4 + (1/2)G(2)
G(2) =
 −4 0
−2 + (1/2)G(1) −4 + (1/2)G(1)
(a) Solve the equations (9) exactly for the values v(1) and v(2).
(b) Carry out Shapley iteration to find v2 starting with the initial guess v0 = (0, 0),
and compare with the exact values found in (a).
II – 75
7. Infinite Games.
In this Chapter, we treat infinite two-person, zero-sum games. These are games
(X, Y, A), in which at least one of the strategy sets, X and Y , is an infinite set. The
famous example of Exercise 4.7.3, he-who-chooses-the-larger-integer-wins, shows that an
infinite game may not have a value. Even worse, the example of Exercise 4.7.5 shows
that the notion of a value may not even make sense in infinite games without further
restrictions. This latter problem will be avoided when we assume that the function A(x, y)
is either bounded above or bounded below.
7.1 The Minimax Theorem for Semi-Finite Games. The minimax theorem for
finite games states that every finite game has a value and both players have optimal mixed
strategies. The first theorem below generalizes this result to the case where only one of
the players has a finite number of pure strategies. The conclusion is that the value exists
and the player with a finite number of pure strategies has an optimal mixed strategy. But
first we must discuss mixed strategies and near optimal strategies for infinite games.
Mixed Strategies for Infinite Games: First note that for infinite games, the notion
of a mixed strategy is somewhat open to choice. Suppose the strategy set, Y , of Player
II is infinite. The simplest choice of a mixed strategy is a finite distribution over Y .
This is a distribution that gives all its probability to a finite number of points. Such a
distribution is described by a finite number of points of Y , say y1, y2,...,yn, and a set of
probabilities, q1, q2,...,qn summing to one with the understanding that point yj is chosen
with probability qj . We will denote the set of finite distributions on Y by Y ∗
F .
When Y is an interval of the real line, we may allow as a mixed strategy any distribution
over Y given by its distribution function, F(z). Here, F(z) represents the probabiity
that the randomly chosen pure strategy, y, is less than or equal to z. The advantage of
enlarging the set of mixed strategies is that it then becomes more likely that an optimal
mixed strategy will exist. The payoff for using such a strategy is denoted by A(x, F) for
x ∈ X.
Near Optimal Strategies for Infinite Games: When a game has a finite value and
an optimal strategy for a player does not exist, that player must be content to choosing
a strategy that comes within  of achieving the value of the game for some small  > 0.
Such a strategy is called an -optimal strategy and was discussed in Chapter 6.
In infinite games, we allow the value to be +∞ or −∞. For example, the value is +∞
if for every number B, however large, there exists a mixed strategy p for Player I such
that A(p, y) ≥ B for all y ∈ Y . A simple example would be: X = [0,∞), Y arbitrary, and
A(x, y) = x independent of y. The value is +∞, since for any B, Player I can guarantee
winning at least B by choosing any x ≥ B. Such a strategy might be called B-optimal.
We will refer to both -optimal and B-optimal strategies as near optimal strategies.
The Semi-Finite Minimax Theorem. For finite X = {x1,...,xm}, we denote the set
of mixed strategies of Player I as usual by X∗. If Player I uses p ∈ X∗ and Player II uses
II – 76
q ∈ Y ∗
F , then the average payoff is denoted by
A(p, q) = 

i


j
piA(xi , yj )qj . (1)
We denote the set of mixed strategies of Player II by Y ∗, but we shall always assume that
Y ∗
F ⊂ Y ∗.
Consider the semi-finite two-person zero-sum game, (X, Y, A), in which X is a finite
set, Y is an arbitrary set, and A(x, y) is the payoff function — the winnings of Player I
if he chooses x ∈ X and Player II chooses y ∈ Y . To avoid the the possibility that the
average payoff does not exist or that the value might be −∞, we assume that the payoff
function, A(x, y), is bounded below. By bounded below, we mean that there is a number
M such that A(x, y) > M for all x ∈ X and all y ∈ Y . This assumption is weak from
the point of view of utility theory because, as mentioned in Appendix 1, it is customary
to assume that utility is bounded.
It is remarkable that the minimax theorem still holds in this situation. Specifically
the value exists and Player I has a minimax strategy. In addition, for every  > 0, Player
II has an -minimax strategy within Y ∗
F .
Theorem 7.1. If X is finite and A is bounded below, then the game (X, Y, A) has a finite
value and Player I has an optimal mixed strategy. In addition, if X has m elements, then
Player II has near optimal strategies that give weight to at most m points of Y .
This theorem is valid without the asumption that A is bounded below provided Player
II is restricted to finite strategies, i.e. Y ∗ = Y ∗
F . See Exercise 1. However, the value may
be −∞, and the notion of near optimal strategies must be extended to this case.
By symmetry, if Y is finite and A is bounded above, then the game (X, Y, A) has a
value and Player II has an optimal mixed strategy.
Solving Semi-Finite Games. Here are two methods that may be used to solve semi-
finite games. We take X to be the finite set, X = {x1,...,xm}.
METHOD 1. The first method is similar to the method used to solve 2 × n games
presented in Section 2.2. For each fixed y ∈ Y , the payoff, A(p, y), is a linear function of p
on the set X∗ = {p = (p1,...,pm) : pi ≥ 0,
pi = 1}. The optimal strategy for Player I is
that value of p that maximizes the lower envelope, f(p) ≡ infy∈Y A(p, y). Note that f(p),
being the infimum of a collection of concave continuous (here linear) functions, is concave
and continuous on X∗. Since X∗ is compact, there exists a p at which the maximum of f(p)
is attained. General methods for solving concave maximization problems are available.
Example 1. Player I chooses x ∈ {x1, x2}, Player II chooses y ∈ [0, 1], and the payoff
is
A(x, y) =  y if x = x1
(1 − y)2 if x = x2
II – 77
A(p, y)
0 1 .553
0
1
p
y=0
y=1/4
y=.382
y=1/2
y=2/3
y=1
Figure 7.1
Let p denote the probability that Player I chooses x1 and let p = (p, 1−p). For a given
choice of y ∈ Y by Player II, the expected payoff to Player I is A(p, y) = py+(1−p)(1−y)2.
The minimum of A(p, y) over y occurs at p − (1 − p)2(1 − y) = 0, or y = (2 − 3p)/(2 − 2p);
except that for p > 2/3, the minimum occurs at y = 0. So, the lower envelope is
f(p) = min
y
A(p, y) =  p 4−5p
4−4p
if p ≤ 2/3
1 − p if p ≥ 2/3
The maximum of this function occurs for p ≤ 2/3, and is easily found to be p = 1 −
(1/
√5) = .553 .... The optimal strategy for Player II occurs at that value of y for which
the slope of A(p, y) (as a function of p) is zero. This occurs when y = (1 − y)2. We find
y = (3 − √5)/2 = .382 ... is an optimal pure strategy for Player II. This is also the value
of the game. See Figure 7.1, in which the lower envelope is shown as the thick line.
METHOD 2: S-GAMES. (Blackwell and Girshick (1954).) Let X = {1, 2,...,m},
and let S be a non-empty convex subset of m-dimensional Euclidean space, Rm, and
assume that S is bounded below. Player II chooses a point s = (s1,...,sm) in S, and
simultaneously Player I chooses a coordinate i ∈ X. Then Player II pays si to Player I.
Such games are called S-games.
This game arises from the semi-finite game (X, Y, A) with X = {1, 2,...,m} by letting
S0 = {s = (A(1, y),...,A(m, y)) : y ∈ Y }. Choosing y ∈ Y is equivalent to choosing
s ∈ S0. Although S0 is not necessarily convex, Player II can, by using a mixed strategy,
choose a probability mixture of points in S0. This is equivalent to choosing a point s in
S, where S is the convex hull of S0.
To solve the game, let Wc denote the “wedge” at the point (c, . . . , c) on the diagonal
in Rm,
Wc = {s : si ≤ c for all i = 1,...,m}.
II – 78
Start with some c such that the wedge Wc contains no points of s, i.e. Wc ∩ S = ∅. Such
a value of c exists from the assumption that S is bounded below. Now increase c and so
push the wedge up to the right until it just touches S. See Figure 7.2(a). This gives the
value of the game:
v = sup{c : Wc ∩ S = ∅}.
Any point s ∈ Wv ∩ S is an optimal pure strategy for Player II. It guarantees that II will
lose no more than v. Such a strategy will exist if S is closed. If Wv ∩ S is empty, then
any point s ∈ Wv+ ∩ S is an -optimal strategy for Player II. The point (v,... , v) is not
necessarily an optimal strategy for Player II. The optimal strategy could be on the side of
the wedge as in Figure 7.2(b).
S
Wv
(v,v)
(0,0)
W S v
(v,v)
(0,0)
Figure 7.2(a) Figure 7.2(b)
To find an optimal strategy for Player I, first find a plane that separates Wv and S,
i.e. that keeps Wv on one side and S on the other. Then find the vector perpendicular to
this plane, i.e. the normal vector. The optimal strategy of Player I is the mixed strategy
with components proportional to this normal vector.
Example 2. Let S be the set S = {(y1, y2) : y1 ≥ 0, y2 ≥ (1−y1)2}. This is essentially
the same as Example 1. The wedge first hits S at the vertex (v, v) when v = (1−v)2. The
solution to this equation gives the value, v = (3 − √5)/2. The point (v, v) is optimal for
Player II. To find Player I’s optimal strategy, we find the slope of the curve y2 = (1−y1)2 at
the point (v, v). The slope of the curve is −2(1−y1), which at y1 = v is −2(1−v)=1−√5.
The slope of the normal is the negative of the reciprocal of this, namely 1/(
√5 − 1). So
p2/p1 = 1/(
√5 − 1), and since p1 + p2 = 1, we find p2(
√5 − 1) = 1 − p2, or p2 = 1/
√5 and
p1 = 1 − (1/
√
5). as found in Example 1.
7.2 Continuous Games. The simplest extension of the minimax theorem to a more
general case is to assume that X and Y are compact subsets of Euclidean spaces, and that
A(x, y) is a continuous function of x and y. To conclude that optimal strategies for the
players exist, we must allow arbitrary distribution functions on X and Y . Thus if X is a
compact subset of m-dimensional space Rm, X∗ is taken to be the set of all distributions
II – 79
on Rm that give probability 0 to the complement of X. Similarly if Y is n-dimensional,
Y ∗ is taken to be the set of all distributions on Rn giving weight 0 to the complement of
Y . Then A is extended to be defined on X∗ × Y ∗ by
A(P, Q) =   A(x, y) dP(x) dQ(y)
Theorem 7.2. If X and Y are compact subsets of Euclidean space and if A(x, y) is a
continuous function of x and y, then the game has a value, v, and there exist optimal
strategies for the players; that is, there is a P0 ∈ X∗ and a Q0 ∈ Y ∗ such that
A(P, Q0) ≤ v ≤ A(P0, Q) for all P ∈ X∗ and Q ∈ Y ∗.
Example 1. Consider the game (X, Y, A), where X = Y = [0, 1], the unit interval,
and
A(x, y) =  g(x − y) if 0 ≤ y ≤ x
g(1 + x − y) if 0 ≤ x<y ≤ 1,
where g is a continuous function defined on [0, 1], with g(0) = g(1). Here, both X∗ and
Y ∗ are the set of probability distributions on the unit interval.
Since X and Y are compact and A(x, y) is continuous on [0, 1]2, we have by Theorem
7.2, that the game has a value and the players have optimal strategies. Let us check that
the optimal strategies for both players is the uniform distribution on [0, 1]. If Player I uses
a uniform on [0,1] to choose x and Player II uses the pure strategy y ∈ [0, 1], the expected
payoff to Player I is
 1
0
A(x, y) dx =
 y
0
g(1 + x − y) dx +
 1
y
g(x − y) dx
=
 1
1−y
g(u) du +
 1−y
0
g(u) du =
 1
0
g(u) du
Since this is independent of y, Player I’s strategy is an equalizer strategy, guaranteeing
him an average payoff of  1
0 g(u) du. Clearly, the same analysis gives Player II this same
amount if he chooses y at random according to a uniform distribution on [0,1]. So these
strategies are optimal and the value is v =  1
0 g(u) du. It may be noticed that this example
is a continuous version of a Latin square game. In fact the same solution holds even if g
in not continuous. One only needs g to be integrable on [0, 1].
A One-Sided Minimax Theorem. In the way that Theorem 7.1 generalized the
finite minimax theorem by allowing Y to be an arbitrary set, Theorem 7.2 may be generalized
to allow Y to be arbitrary, provided we keep the compactness condition on X.
The continuity condition may be weakened to assuming only that A(x, y) is a continuous
function of x for every y ∈ Y . And even this can be weakened to assuming that A(x, y) is
only upper semi-continuous in x for every y ∈ Y .
II – 80
A function f(x) defined on X is upper semi-continuous at a point x0 ∈ X, if for any
sequence x1, x2,... of points in X such that limn→∞ xn = x0, we have limn→∞ f(xn) ≤
f(x0). It is upper semi-continuous (usc) on X if it is upper semicontinuous at every point
of X. A function f(x) is lower semi-continuous (lsc) if the above inequality is changed to
limn→∞ f(xn) ≥ f(x0), or equivalently, if the function, −f(x), is upper semi-continuous.
As an example, the function
f(x) =  0 if x < 0
a if x = 0
1 if x > 0
is usc if a ≥ 1 and lsc if a ≤ 0. It is neither usc nor lsc if 0 <a< 1.
Theorem 7.3. If X is a compact subset of Euclidean space, and if A(x, y) is an upper
semi-continuous function of x ∈ X for all y ∈ Y and if A is bounded below (or if Y ∗ is the
set of finite mixtures), then the game has a value, Player I has an optimal strategy in X∗,
and for every  > 0 Player II has an -optimal strategy giving weight to a finite number of
points.
Similarly from Player II’s viewpoint, if Y is a compact subset of Euclidean space, and
if A(x, y) is a lower semi-continuous function of y ∈ Y for all x ∈ X and if A is bounded
above (or if X∗ is the set of finite mixtures), then the game has a value and Player II has
an optimal strategy in Y ∗.
Example 2. Player I chooses a number in [0,1] and Player II tries to guess what it
is. Player I wins 1 if Player II’s guess is off by at least 1/3; otherwise, there is no payoff.
Thus, X = Y = [0, 1], and A(x, y) =  1 if |x − y| ≥ 1/3
0 if |x − y| < 1/3. Although the payoff
function is not continuous, it is upper semi-continuous in x for every y ∈ Y . Thus the
game has a value and Player I has an optimal mixed strategy.
If we change the payoff so that Player I wins 1 if Player II’s guess is off by more than
1/3, then A(x, y) =  1 if |x − y| > 1/3
0 if |x − y| ≤ 1/3. This is no longer upper semi-continuous in x
for fixed y; instead it is lower semi-continuous in y for each x ∈ X. This time, the game
has a value and Player II has an optimal mixed strategy.
7.3 Concave Games and Convex Games. If in Theorem 7.2, we add the assumption
that the payoff function A(x, y) is concave in x for all y or convex in y for all x, then
we can conclude that one of the players has an optimal pure strategy, which is usually easy
to find. Here is a one-sided version that complements Theorem 7.3. A good reference for
these ideas is the book of Karlin (1959), vol. 2.
Theorem 7.4. Let (X, Y, A) be a game with Y arbitrary, X a compact, convex subset of
Rm, and A(x, y) bounded below. If A(x, y) is a concave function of x ∈ X for all y ∈ Y ,
then the game has a value and Player I has an optimal pure strategy. Moreover, Player II
has an -optimal strategy that is a mixture of at most m + 1 pure strategies.
II – 81
The dual statement for convex functions is: If Y is compact and convex in Rn, and
if A is bounded above and is convex in y ∈ Y for all x ∈ X, then the game has a value,
Player II has an optimal pure strategy and Player I has -optimal strategies giving weight
to at most n + 1 points..
These games may be solved by a method similar to Method 1 of Section 7.1. Let’s
see how to find the optimal strategy of Player II in the convex functio case. Let g(y) =
supx A(x, y) be the upper envelope. Then g(y) is finite since A is bounded above. It is also
convex since the supremum of any set of convex functions is convex. Then since convex
functions defined on a compact set attain their maximum, there exists a point y∗ at which
g(y) takes on its maximum value, so that
A(x, y∗) ≤ max x A(x, y∗) = g(y∗) for all x ∈ X.
Any such point is an optimal pure strategy for Player II. By choosing y∗, Player II will
lose no more than g(y∗) no matter what Player I does. Player I’s optimal strategy is
more complex to describe in general; it gives weight only to points that play a role in the
upper envelope at the point y∗. These are points x such that A(x, y) is tangent (or nearly
tangent if only -optimal strategies exist) to the surface g(y) at y∗. It is best to consider
examples.
Example 1. Estimation. Player I chooses a point x ∈ X = [0, 1], and Player II
tries to choose a point y ∈ Y = [0, 1] close to x. Player II loses the square of the distance
from x to y: A(x, y)=(x − y)2. This is a convex function of y ∈ [0, 1] for all x ∈ X.
For any x, A(x, y) is bounded above by either A(0, y) or A(1, y) so the upper envelope
is g(y) = max{A(0, y), A(1, y)} = max{y2,(1 − y)2}. This is minimized at y∗ = 1/2. If
Player II uses y∗, she is guaranteed to lose no more than g(y∗)=1/4.
Since x = 0 and x = 1 are the only two pure strategies influencing the upper envelope,
and since y2 and (1−y)2 have slopes at y∗ that are equal in absolute value but opposite in
sign, Player I should mix 0 and 1 with equal probability. This mixed strategy has convex
payoff (1/2)(A(0, y) + A(1, y)) with slope zero at y∗. Player I is guaranteed winning at
least 1/4, so v = 1/4 is the value of the game. The pure strategy y∗ is optimal for Player
II and the mixed strategy, 0 with probability 1/2 and 1 with probability 1/2, is optimal for
Player I. In this example, n = 1, and Player I’s optimal strategy mixes 2 = n + 1 points.
Theorem 7.4 may also be stated with the roles of the players reversed. If Y is arbitrary,
and if X is a compact subset of Rm and if A(x, y) is bounded below and concave in x ∈ X
for all y ∈ Y , then Player I has an optimal pure strategy, and Player II has an -optimal
strategy mixing at most m +1 pure strategies. It may also happen that A(x, y) is concave
in x for all y, and convex in y for all x. In that case, both players have optimal pure
strategies as in the following example.
Example 2. A Convex-Concave Game. Suppose X = Y = [0, 1], and A(x, y) =
−2x2 +4xy+y2 −2x−3y+1. The payoff is convex in y for all x and concave in x for all y.
Therefore, both players have pure optimal strategies, say x0 and y0. If Player II uses y0,
II – 82
then A(x, y0) must be maximized by x0. To find maxx∈[0,1] A(x, y0) we take a derivative
with respect to x: ∂
∂xA(x, y0) = −4x + 4y0 − 2. So
x0 =
 y0 − (1/2) if y0 > 1/2
0 if y0 ≤ 1/2
Similarly, if Player I uses x0, then A(x0, y) is minimized by y0. Since ∂
∂y A(x0, y)=4x0 +
2y − 3, we have
y0 =
⎧
⎨
⎩
1 if x0 ≤ 1/4
(1/2)(3 − 4x0) if 1/4 ≤ x0 ≤ 3/4
0 if x0 ≥ 3/4.
These two equations are satisfied only if x0 = y0 − (1/2) and y0 = (1/2)(3 − 4x0). It is
then easily found that x0 = 1/3 and y0 = 5/6. The value is A(x0, y0) = −7/12.
It may be easier here to find the saddle-point of the surface, z = −2x2 + 4xy + y2 −
2x − 3y + 1, and if the saddle-point is in the unit square, then that is the solution. But
the method used here shows what must be done in general.
7.4 Solving Games. There are many interesting games that are more complex and
that require a good deal of thought and ingenuity to find solutions. There is one tool
for solving such games that is basic. This is the infinite game analog of the principle of
indifference given in Chapter 3: Search for strategies that make the opponent indifferent
among all his “good” pure strategies.
To be more specific, consider the game (X, Y, A) with X = Y = [0, 1] and A(x, y)
continuous. Let v denote the value of the game and let P denote the distribution that
represents the optimal strategy for Player I. Then, A(P, y) must be equal to v for all
“good” y, which here means for all y in the support of Q for any Q that is optimal for
Player II. (A point y is in the support of Q if the Q probability of the interval (y −, y +)
is positive for all  > 0.) So to attempt to find the optimal P, we guess at the set, S, of
“good” points y for Player II and search for a distribution P such that A(P, y) is constant
on S. Such a strategy, P, is called an equalizer strategy on S. The first example shows
what is involved in this.
Example 1. Meeting Someone at the Train Station. A young lady is due to
arrive at a train station at some random time, T, distributed uniformly between noon and
1 PM. She is to wait there until one of her two suitors arrives to pick her up. Each suitor
chooses a time in [0,1] to arrive. If he finds the young lady there, he departs immediately
with her; otherwise, he leaves immediately, disappointed. If either suitor is successful in
meeting the young lady, he receives 1 unit from the other. If they choose the same time
to arrive, there is no payoff. Also, if they both arrive before the young lady arrives, the
payoff is zero. (She takes a taxi at 1 PM.)
Solution: Denote the suitors by I and II, and their strategy spaces by X = [0, 1]
and Y = [0, 1]. Let us find the function A(x, y) that represents I’s expected winnings if I
chooses x ∈ X and II chooses y ∈ Y . If x<y, I wins 1 if T <x and loses 1 if x<T <y.
II – 83
The probability of the first is x and the probability of the second is y − x, so A(x, y) is
x − (y − x)=2x − y when x<y. When y<x, a similar analysis shows A(x, y) = x − 2y,
Thus,
A(x, y) =  2x − y if x<y
x − 2y if x>y
0 if x = y.
(1)
This payoff function is not continuous, nor is it upper semicontinuous or lower semicontinuous.
It is symmetric in the players so if it has a value, the value is zero and the players
have the same optimal strategy.
Let us search for an equalizer strategy for Player I and assume it has a density f(x)
on [0,1]. We would have
A(f,y) =  y
0
(2x − y)f(x) dx +
 1
y
(x − 2y)f(x) dx
=
 y
0
(x + y)f(x) dx +
 1
0
(x − 2y)f(x) dx = constant
(2)
Taking a derivative with respect to y yields the equation
2yf(y) +  y
0
f(x) dx − 2
 1
0
f(x) dx = 0 (3)
and taking a second derivative gives
2f(y)+2yf
(y) + f(y) = 0 or f
(y)
f(y) = − 3
2y
. (4)
This differential equation has the simple solution,
log f(y) = −3
2 log(y) + c or f(y) = ky−3/2 (5)
for some constants c and k. Unfortunately,  1
0 y−3/2 dy = ∞, so this cannot be used as a
density on [0,1].
If we think more about the problem, we can see that it cannot be good to come in very
early. There is too little chance that the young lady has arrived. So perhaps the “good”
points are only those from some point a > 0 on. That is, we should look for a density
f(x) on [a, 1] that is an equalizer from a on. So in (2) we replace the integrals from 0 to
integrals from a and assume y>a. The derivative with respect to y gives (3) with the
integrals starting from a rather than 0. And the second derivative is (4) exactly. We have
the same solution (5) but for y>a. This time the resulting f(y) on [a, 1] is a density if
k−1 =
 1
a
x−3/2 dx = −2
 1
a
dx−1/2 = 2(1 − √a)
√a (6)
II – 84
We now need to find a. That may be done by solving equation (3) with the integrals
starting at a.
2yky−3/2 +
 y
a
kx−3/2 dx − 2=2ky−1/2 − 2k(y−1/2 − a−1/2) − 2=2ka−1/2 − 2=0
So ka−1/2 = 1, which implies 1 = 2(1 − √a) or a = 1/4, which in turn implies k = 1/2.
The density
f(x) =  0 if 0 <x< 1/4
(1/2)x−3/2 if 1/4 <x< 1 (7)
is an equalizer for y > 1/4 and is therefore a good candidate for the optimal strategy. We
should still check at points y less than 1/4. For y < 1/4, we have from (2) and (7)
A(f,y) =  1
1/4
(x − 2y)(1/2)x−3/2 dx =
 1
1/4
1
2
√x
dx − 2y = 1
2 − 2y.
So
A(f,y) =  (1 − 4y)/2 for y < 1/4
0 for y > 1/4 (8)
This guarantees I at least 0 no matter what II does. Since II can use the same strategy,
The value of the game is 0 and (7) is an optimal strategy for both players.
Example 2. Competing Investors. Two investors compete to see which of them,
starting with the same initial fortune, can end up with the larger fortune. The rules of
the competition require that they invest only in fair games. That is, they can only invest
non-negative amounts in games whose expected return per unit invested is 1.
Suppose the investors start with 1 unit of fortune each (and we assume money is
infinitely divisible). Thus no matter what they do, their expected fortune at the end is
equal to their initial fortune, 1.
Thus the players have the same pure strategy sets. They both choose a distribution
on [0,∞) with mean 1, say Player I chooses F with mean 1, and Player II chooses G with
mean 1. Then Z1 is chosen from F and Z2 is chosen from G independently, and I wins
if Z1 > Z2, II wins if Z2 > Z1 and it is a tie if Z1 = Z2. What distributions should the
investors choose?
The game is symmetric in the players, so the value if it exists is zero, and both players
have the same optimal strategy. Here the strategy spaces are very large, much larger than
in the Euclidean case. But it turns out that the solution is easy to describe. The optimal
strategy for both players is the uniform distribution on the interval (0,2):
F(z) =  z/2 for 0 ≤ z ≤ 2
1 for z > 2
This is a distribution on [0,∞] with mean 1 and so it is an element of the strategy space
of both players. Suppose Player I uses F. Then the probability that I loses is
P(Z1 < Z2) = E[P(Z1 < Z2|Z2)] ≤ E[Z2/2] = (1/2)E[Z2]=1/2.
II – 85
So the probability I wins is at least 1/2. Since the game is symmetric, Player II by using
the same strategy can keep Player I’s probability of winning to at most 1/2.
7.5 Uniform[0,1] Poker Models. The study of two-person Uniform[0,1] poker
models goes back to Borel (1938) and von Neumann (1944). We present these two models
here. In these models, the set of possible “hands” of the players is the interval, [0, 1].
Players I and II are dealt hands x and y respectively in [0, 1] according to a uniform
distribution over the interval [0, 1]. Throughout the play, both players know the value of
their own hand, but not that of the opponent. We assume that x and y are independent
random variables; that is, learning the value of his own hand gives a player no information
about the hand of his opponent.
There follows some rounds of betting in which the players take turns acting. After
the dealing of the hands, all actions that the players take are announced. Except for the
dealing of the hands at the start of the game, this would be a game of perfect information.
Games of this sort, where, after an initial random move giving secret information to the
players, the game is played with no further random moves of nature, are called games of
almost perfect information (See Sorin and Ponssard (1980).
It is convenient to study the action part of games of almost complete information by
what we call the betting tree. This is distinct from the Kuhn tree in that it neglects the
information sets that may arise from the initial distribution of hands. The examples below
illustrate this concept.
The Borel Model: La Relance. Both players contribute an ante of 1 unit into the
pot and receive independent uniform hands on the interval [0, 1]. Player I acts first either
by folding and thus conceding the pot to Player II, or by betting a prescribed amount
β > 0 which he adds to the pot. If Player I bets, then Player II acts either by folding and
thus conceding the pot to Player I, or by calling and adding β to the pot. If Player II calls
the bet of Player I, the hands are compared and the player with the higher hand wins the
entire pot. That is, if x>y then Player I wins the pot; if x<y then Player II wins the
pot. We do not have to consider the case x = y since this occurs with probability 0.
The betting tree is
I
II
bet fold
call fold
+1
−1
±(β +1)
In this diagram, the plus-or-minus sign indicates that the hands are compared, and
the higher hand wins the amount β + 1.
II – 86
It is easy to see that the optimal strategy for Player II must be of the form for some
number b in the interval [0,1]: fold if y<b and call if y>b. The optimal value of b may be
found using the principle of indifference. Player II chooses b to make I indifferent between
betting and folding when I has some hand x<b. If I bets with such an x, he, wins 2 (the
pot) if II has y<b and loses β if II has y>b. His expected winnings are in this case,
2b − β(1 − b). On the other hand, if I folds he wins nothing. (This views the game as a
constant-sum game. It views the money already put into the pot as a sunk cost, and so
the sum of the payoffs of the players is 2 whatever the outcome. This is a minor point but
it is the way most poker players view the pot.) He will be indifferent between betting and
folding if
2b − β(1 − b)=0
from which we conclude
b = β/(2 + β). (1)
Player I’s optimal strategy is not unique, but all of his optimal strategies are of the
form: if x>b, bet; and if x<b, do anything provided the total probability that you fold
is b2. For example, I may fold with his worst hands, i.e. with x<b2, or he may fold with
the best of his hands less than b, i.e. with b − b2 <x<b, or he may, for all 0 <x<b,
simply toss a coin with probability b of heads and fold if the coin comes up heads.
The value of the game may be computed as follows. Suppose Player I folds with any
x<b2 and bets otherwise and suppose Player II folds with y<b. Then the payoff in the
unit square has the values given in the following diagram. The values in the upper right
corner cancel and the rest is easy to evaluate. The value is v(β) = −(β + 1)(1 − b)(b −
b2) + (1 − b2)b − b2, or, recalling b = β/(2 + β),
v(β) = −b2 = − β2
(2 + β)2 . (2)
Thus, the game is in favor of Player II.
0
0
0
0
1
1
y
x
b
b b 2
−1
+1
−(β+1)
−(β+1)
β+1
II – 87
We summarize in
Theorem 7.5. The value of la relance is given by (2). An optimal strategy for Player I
is to bet if x>b − b2 and to fold otherwise, where b is given in (1). An optimal strategy
for Player II is to call if y>b and to fold otherwise.
As an example, suppose β = 2, where the size of the bet is the size of the pot. Then
b = 1/2. An optimal strategy for Player I is to bet if x > 1/4 and fold otherwise; the
optimal strategy of Player II is to call if y > 1/2. The game favors Player II, whose
expected return is 1/4 unit each time the game is played.
If I bets when x<b, he knows he will lose if called, assuming II is using an optimal
strategy. Such a bet is called a bluff. In la relance, it is necessary for I to bluff with
probability b2. Which of the hands below b he chooses to bluff with is immaterial as far as
the value of the game is concerned. However, there is a secondary advantage to bluffing
(betting) with the hands just below b, that is, with the hands from b2 to b. Such a strategy
takes maximum advantage of a mistake the other player may make.
A given strategy σ for a player is called a mistake if there exists an optimal strategy
for the opponent when used against σ gives the opponent an expected payoff better than
the value of the game. In la relance, it is a mistake for Player II to call with some y<b
or to fold with some y>b. If II calls with some y<b, then I can gain from the mistake
most profitably if he bluffs only with his best hands below b.
A strategy is said to be admissible for a player if no other strategy for that player
does better against one strategy of the opponent without doing worse against some other
strategy of the opponent. The rule of betting if and only if x>b2 is the unique admissible
optimal strategy for Player I.
The von Neumann Model. The model of von Neumann differs from the model of
Borel in one small but significant respect. If Player I does not bet, he does not necessarily
lose the pot. Instead the hands are immediately compared and the higher hand wins the
pot. We say Player I checks rather than folds. This provides a better approximation to
real poker and a clearer example of the concept of “bluffing” in poker. The betting tree of
von Neumann’s poker is the same as Borel’s except that the −1 payoff on the right branch
is changed to ±1.
I
II
bet check
call fold
+1
±1
±(β +1)
II – 88
This time it is Player I that has a unique optimal strategy. It is of the form for some
numbers a and b with a<b: bet if x<a or if x>b, and check otherwise. Although there
are many optimal strategies for Player II (and von Neumann finds all of them), one can
show that there is a unique admissible one and it has the simple form: call if y>c for
some number c. It turns out that 0 <a<c<b< 1.
I: | bet | check | bet |
0a b 1
II: | fold | call |
0 c1
The region x<a is the region in which Player I bluffs. It is noteworthy that Player
I must bluff with his worst hands, and not with his moderate hands. It is a mistake for
Player I to do otherwise. Here is a rough explanation of this somewhat counterintuitive
feature. Hands below c may be used for bluffing or checking. For bluffing it doesn’t matter
much which hands are used; one expects to lose them if called. For checking though it
certainly matters; one is better off checking with the better hands.
Let us apply the principle of indifference to find the optimal values of a, b and c. This
will lead to three equations in three unknowns, known as the indifference equations (not
to be confused with difference equations). First, Player II should be indifferent between
folding and calling with a hand y = c. Again we use the gambler’s point of view of the
game as a constant sum game, where winning what is already in the pot is considered as
a bonus. If II folds, she wins zero. If she calls with y = c, she wins (β + 2) if x<a and
loses β if x>b. Equating her expected winnings gives the first indifference equation,
(β + 2)a − β(1 − b)=0. (3)
Second, Player I should be indifferent between checking and betting with x = a. If
he checks with x = a, he wins 2 if y<a, and wins nothing otherwise, for an expected
return of 2a. If he bets, he wins 2 if y<c and loses β if y>c, for an expected return of
2c − β(1 − c). Equating these gives the second indifference equation,
2c − β(1 − c)=2a. (4)
Third, Player I should be indifferent between checking and betting with x = b. If he
checks, he wins 2 if y<b. If he bets, he wins 2 if y<c and wins β + 2 if c<y<b, and
loses β if y>b, for an expected return of 2c + (β + 2)(b − c) − β(1 − b). This gives the
third indifference equation,
2c + (β + 2)(b − c) − β(1 − b)=2b,
which reduces to
2b − c = 1. (5)
II – 89
The optimal values of a, b and c can be found by solving equations (4) (5) and (6) in
terms of β. The solution is
a = β
(β + 1)(β + 4) b = β2 + 4β + 2
(β + 1)(β + 4) c = β(β + 3)
(β + 1)(β + 4). (6)
The value is
v(β) = a = β/((β + 1)(β + 4)). (7)
This game favors Player I. We summarize this in
Theorem 7.6. The value of von Neumann’s poker is given by (7). An optimal strategy
for Player I is to check if a<x<b and to bet otherwise, where a and b are given in (6).
An optimal strategy for Player II is to call if y>c and to fold otherwise, where c is given
in (6).
For pot-limit poker where β = 2, we have a = 1/9, b = 7/9, and c = 5/9, and the
value is v(2) = 1/9.
It is interesting to note that there is an optimal bet size for Player I. It may be found
by setting the derivative of v(β) to zero and solving the resulting equation for β. It is
β = 2. In other words, the optimal bet size is the size of the pot, as in pot-limit poker!
7.6 Exercises.
1. Let X = {−1, 1}, let Y = {..., −2, −1, 0, 1, 2,...} be the set of all integers, and let
A(x, y) = xy.
(a) Show that if we take Y ∗ = Y ∗
F , the set of all finite distributions on Y , then the
value exists, is equal to zero and both players have optimal strategies.
(b) Show that if Y ∗ is taken to be the set of all distributions on Y , then we can’t speak
of the value, because Player II has a strategy, q, for which the expected payoff, A(x, q)
doesn’t exist for any x ∈ X.
2. Simultaneously, Player I chooses x ∈ {x1, x2}, and Player II chooses y ∈ [0, 1]; then
I receives
A(x, y) =  y if x = x1
e−y if x = x2
from II. Find the value and optimal strategies for the players.
3. Player II chooses a point (y1, y2) in the ellipse (y1 − 3)2 + 4(y2 − 2)2 ≤ 4. Simultaneously,
Player I chooses a coordinate k ∈ {1, 2} and receives yk from Player II. Find the
value and optimal strategies for the players.
4. Solve the two games of Example 2. Hint: Use domination to remove some pure
strategies.
II – 90
5. Consider the game with X = [0, 1], Y = [0, 1], and
A(x, y) =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
0 if x = y
−1 if x = 0 and y > 0
+1 if y = 0 and x > 0
−1 if 0 <y<x
+1 if 0 <x<y
Note that A(x, y) is not usc in x for all y nor lsc in y for all x. Show the game does not
have a value.
6. The Greedy Game. Each player can demand from the other as much as desired
between zero and one, but there is a penalty for being too greedy. The player who demands
more than his opponent must pay a fine of b to the other, where b is a fixed number,
0 ≤ b ≤ 1/2. Thus we have the game (X, Y, A) where X = Y = [0, 1], and
A(x, y) = x − y +
 +b if x<y
0 if x = y
−b if x>y
Solve.
7. Find optimal strategies and the value of the following games.
(a) X = Y = [0, 1] and A(x, y) =  (x − y)2 if x ≤ y
2(x − y)2 if x ≥ y. (Underestimation is the more
serious error of Player II.)
(b) X = Y = [0, 1] and A(x, y) = xe−y + (1 − x)y.
8. Hide and Seek in a Compact, Convex Set. (a) Let S be the triangle in the
plane with vertices (−1, 0), (1, 0), and (0, 2). Player I chooses a point x in S in which to
hide, and Player II chooses a point y in S to seek. The payoff to Player I is the square
of Euclidean distance between x and y. Thus, X = S, Y = S, and A(x, y) = x − y2.
Solve.
(b) See if you can formulate a procedure for solving the above game of Hide and Seek
if S is an arbitrary compact, convex set in Rn.
9. The Wallet Game. Two players each put a random amount with mean one into
their wallets. The player whose wallet contains the smaller amount wins the larger amount
from the opponent.
Carroll, Jones and Rykken (2001) show that this game does not have a value. But
suppose we restrict the players to putting at most some amount b in their wallets. Here is
the game:
Player I, resp. Player II, chooses a distribution F, resp. G, on the interval [0, b] with
mean 1, where b > 1. Then independent random variables, X from F and Y from G, are
II – 91
chosen. If X<Y , Player I wins Y from Player II. If X>Y , Player II wins X from Player
I, and if X = Y , there is no payoff. So the payoff function is
A(F, G) = E(Y I(X<Y ) − XI(X>Y ))
= E((Y + X)I(X<Y )) − 1 + E(XI(X = Y )). (1)
The game is symmetric, so if the value exists, the value is zero, and the players have the
same optimal strategies. Find an optimal strategy for the players. Hint: Search among
distributions F having a density f on the interval (a, b) for some a < 1. Note that the last
term on the right of Equation (1) disappears for such distributions.
10. The Multiplication Game. (See Kent Morrison (2010).) Players I and II
simutaneously select positive numbers x and y. Player I wins +1 if the product xy, written
in decimal form has initial significant digit 1, 2 or 3. Thus, the pure strategy spaces are
X = Y = (0,∞) and the payoff function is
A(x, y) =  +1 if the initial significant digit is 1, 2 or 3
0 otherwise.
Solve.
Hint:(1) First note that both players may restrict their pure strategy sets to X =
Y = [1, 10). so that
A(x, y) = I{1 ≤ xy < 4 or 10 ≤ xy < 40}.
(2) Take logs to the base 10. Let u = log10(x) and v = log10(y). Now, players I and
II choose u and v in [0,1) with payoff
B(u, v) = I{0 ≤ u + v<c or 1 ≤ u + v < 1 + c}
where c = log10(4) = .60206 .... Solve the game in this form and translate back to the
original game.
11. Suppose, in La Relance, that when Player I checks, Player II is given a choice
between checking in which case there is no payoff, and calling in which case the hands are
compared and the higher hand wins the antes.
(a) Draw the betting tree.
(b) Assume optimal strategies of the following form. I checks if and only if 0 <a<
x<b< 1 for some a and b. If I bets, then II calls iff y>c, and if Player I checks, Player
II calls iff y>d, where a ≤ c ≤ b and a ≤ d ≤ b. Find the indifference equations.
(c) Solve the equations when β = 2, and find the value in this case. Which player has
the advantage?
12. Last Round Betting. Here is a game that occurs in the last round of blackjack
or baccarat tournaments, and also in the television game show, Final Jeopardy. For the
general game, see Ferguson and Melolidakis (1997).
II – 92
In the last round of betting in a contest to see who can end up with the most money,
Player I starts with $70 and Player II starts with $100. Simultaneously, Player I must
choose an amount to bet between $0 and $70 and Player II must choose an amount between
$0 and $100. Then the players independently play games with probability .6 of winning
the bet and .4 of losing it. The player who has the most money at the end wins a big prize.
If they end up with the same amount of money, they share the prize.
We may set this up as a game (X, Y, A), with X = [0, 0.7], Y = [0, 1.0], measured in
units of $100, and assuming money is infinitely divisible. We assume the payoff, A(x, y),
is the probability that Player I wins the game plus one-half the probabiity of a tie, when
I bets x and II bets y. The probability that both players win their bets is .6 ∗ .6 = .36,
the probability that both players lose their bets is .4 ∗ .4 = .16, and the probability that I
wins his bet and II loses her bet is .6 ∗ .4 = .24. Therefore,
P(I wins) = .36 I(.7 + x > 1 + y) + .24 I(.7 + x > 1 − y) + .16 I(.7 − x > 1 − y)
= .36 I(x − y>.3) + .24 I(x + y>.3) + .16 I(y − x>.3)
P(a tie) = .36 I(.7 + x =1+ y) + .24 I(.7 + x = 1 − y) + .16 I(.7 − x = 1 − y)
= .36 I(x − y = .3) + .24 I(x + y = .3) + .16 I(y − x = .3)
where I(·) represents the indicator function. This gives
A(x, y) = P(I wins) + 1
2
P(a tie) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
.60 if y<x − .3
.40 if y>x + .3
.00 if y + x<.3
.24 if y>x − .3,y<x + .3, y + x>.3
.42 if 0 < y = x − .3
.32 if 0 < x = y − .3
.12 if x + y = .3,x> 0,y> 0
.30 if x = .3, y = 0
.20 if x = 0, y = .3
Find the value of the game and optimal strategies for both players. (Hint: Both players
have an optimal strategy that give probability to only two points.)
II – 93
References.
Robert J. Aumann and Michael B. Maschler (1995) Repeated Games of Incomplete Information,
The MIT Press, Cambridge, Mass.
A. Ba˜nos (1968) On pseudo-games. Ann. Math. Statist. 39, 1932-1945.
V. J. Baston, F. A. Bostock and T. S. Ferguson (1989) The number hides game, Proc.
Amer. Math. Soc. 107, 437-447.
John D. Beasley (1990) The Mathematics of Games, Oxford University Press.
Emile Borel (1938) Trait´ ´ e du Calcul des Probabilit´es et ses Applications Volume IV, Fascicule
2, Applications aux jeux des hazard, Gautier-Villars, Paris.
G. W. Brown (1951) ”Iterative Solutions of Games by Fictitious Play” in Activity Analysis
of Production and Allocation, T.C. Koopmans (Ed.), New York: Wiley.
G. S. Call and D. J. Velleman (1993) Pascal’s matrices, Amer. Math. Mo. 100, 372-376.
M. T. Carroll, M. A. Jones, E. K.Rykken (2001) The Wallet Paradox Revisited, Math.
Mag. 74, 378-383.
W. H. Cutler (1975) An optimal strategy for pot-limit poker, Amer. Math. Monthly 82,
368-376.
W. H. Cutler (1976) End-Game Poker, Preprint.
Melvin Dresher (1961) Games of Strategy: Theory and Applications, Prentice Hall, Inc.
N.J.
Melvin Dresher (1962) A sampling inspection problem in arms control agreements: a gametheoretic
analysis, Memorandum RM-2972-ARPA, The RAND Corporation, Santa
Monica, California.
R. J. Evans (1979) Silverman’s game on intervals, Amer. Math. Mo. 86, 277-281.
H. Everett (1957) Recursive games, Contrib. Theor. Games III, Ann. Math. Studies 39,
Princeton Univ. Press, 47-78.
C. Ferguson and T. Ferguson (2007) The Endgame in Poker, in Optimal Play: Mathematical
Studies of Games and Gambling, Stuart Ethier and William Eadington, eds.,
Institute for the Study of Gambling and Commercial Gaming, 79-106.
T. S. Ferguson (1967) Mathematical Statistics — A Decision-Theoretic Approach, Academic
Press, New York.
T. S. Ferguson and C. Melolidakis (1997) Last Round Betting, J. Applied Probability 34
974-987.
J. Filar and K. Vrieze (1997) Competitive Markov Decision Processes, Springer-Verlag,
New York.
L. Friedman (1971) Optimal bluffing strategies in poker, Man. Sci. 17, B764-B771.
II – 94
S. Gal (1974) A discrete search game, SIAM J. Appl. Math. 27, 641-648.
M. Gardner (1978) Mathematical Magic Show, Vintage Books, Random House, New York.
Andrey Garnaev (2000) Search Games and Other Applications of Game Theory, Lecture
Notes in Economics and Mathematical Systems 485, Springer.
G. A. Heuer and U. Leopold-Wildburger (1991) Balanced Silverman Games on General
Discrete Sets, Lecture Notes in Econ. & Math. Syst., No. 365, Springer-Verlag.
R. Isaacs (1955) A card game with bluffing, Amer. Math. Mo. 62, 99-108.
S. M. Johnson (1964) A search game, Advances in Game Theory, Ann. Math. Studies 52,
Princeton Univ. Press, 39-48.
Samuel Karlin (1959) Mathematical Methods and Theory in Games, Programming and
Economics, in two vols., Reprinted 1992, Dover Publications Inc., New York.
H. W. Kuhn, (1950) A simplified two-person poker, Contrib. Theor. Games I, Ann. Math.
Studies 24, Princeton Univ. Press, 97-103.
H. W. Kuhn (1997) Classics in Game Theory, Princeton University Press.
A. Maitra and W. Sudderth (1996) Discrete Gambling and Stochastic Games, in the Series
Applications in Mathematics 32, Springer.
J. J. C. McKinsey (1952) Introduction to the Theory of Games, McGraw-Hill, New York.
N. S. Mendelsohn (1946) A psychological game, Amer. Math. Mo. 53, 86-88.
J. Milnor and L. S. Shapley (1957) On games of survival, Contrib. Theor. Games III, Ann.
Math. Studies 39, Princeton Univ. Press, 15-45.
Kent E. Morrison (2010) The Multiplication Game, Math. Mag. 83, 100-110.
J. F. Nash and L. S. Shapley (1950) A simple 3-person poker game, Contrib. Theor. Games
I, Ann. Math. Studies 24, Princeton Univ. Press, 105-116.
D. J. Newman (1959) A model for “real” poker, Oper. Res. 7, 557-560.
Guillermo Owen (1982) Game Theory, 2nd Edition, Academic Press.
T. E. S. Raghavan, T. S. Ferguson, T. Parthasarathy and O. J. Vrieze, eds. (1991) Stochastic
Games and Related Topics, Kluwer Academic Publishers.
J. Robinson (1951) An Iterative Method of Solving a Game, Annals of Mathematics 54,
296-301.
W. H. Ruckle (1983) Geometric games and their applications, Research Notes in Mathematics
82, Pitman Publishing Inc.
L. S. Shapley (1953) Stochastic Games, Proc. Nat. Acad. Sci. 39, 1095-1100.
L. S. Shapley and R. N. Snow (1950) Basic solutions of discrete games, Contrib. Theor.
Games I, Ann. Math. Studies 24, Princeton Univ. Press, 27-35.
II – 95
S. Sorin and J. P. Ponssard (1980) The LP formulation of finite zero-sum games with
incomplete information, Int. J. Game Theory 9, 99-105.
Philip D. Straffin (1993) Game Theory and Strategy, Mathematical Association of America.
John Tukey (1949) A problem in strategy, Econometrica 17, 73.
J. von Neumann and O. Morgenstern (1944) The Theory of Games and Economic Behavior,
Princeton University Press.
J. D. Williams, (1966) The Compleat Strategyst, 2nd Edition, McGraw-Hill, New York.
II – 96
